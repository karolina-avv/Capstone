{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971bf115-0d70-4a81-94dc-befa8bd4f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc8ee91f-3b49-464d-9d0d-4cc0278a5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Data\n",
    "# Replace 'path_to_train_file.csv' and 'path_to_test_file.csv' with the actual file paths\n",
    "train_data = pd.read_csv('/Users/marlenawasiak/Desktop/Data_Collection/NSL_KDD_Train.csv')\n",
    "test_data = pd.read_csv('/Users/marlenawasiak/Desktop/Data_Collection/NSL_KDD_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8379b4-68c3-4969-8b3a-6469e34bb771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['apache2' 'back' 'buffer_overflow' 'ftp_write' 'guess_passwd'\n",
      " 'httptunnel' 'imap' 'ipsweep' 'land' 'loadmodule' 'mailbomb' 'mscan'\n",
      " 'multihop' 'named' 'neptune' 'nmap' 'normal' 'perl' 'phf' 'pod'\n",
      " 'portsweep' 'processtable' 'ps' 'rootkit' 'saint' 'satan' 'sendmail'\n",
      " 'smurf' 'snmpgetattack' 'snmpguess' 'spy' 'sqlattack' 'teardrop'\n",
      " 'udpstorm' 'warezclient' 'warezmaster' 'worm' 'xlock' 'xsnoop' 'xterm']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Separate Features and Labels\n",
    "# Assuming the last column is the label and the rest are features\n",
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "# Step 3: Align Columns to Keep Only the Common Columns\n",
    "common_columns = X_train.columns.intersection(X_test.columns)\n",
    "X_train = X_train[common_columns]\n",
    "X_test = X_test[common_columns]\n",
    "\n",
    "# Step 4: Encode Categorical Features with Consistent Categories\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in categorical_columns:\n",
    "    combined_categories = pd.concat([X_train[col], X_test[col]], axis=0).astype(\"category\").cat.categories\n",
    "    X_train[col] = pd.Categorical(X_train[col], categories=combined_categories)\n",
    "    X_test[col] = pd.Categorical(X_test[col], categories=combined_categories)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    X_test[col] = le.transform(X_test[col].astype(str))\n",
    "\n",
    "# Step 5: Scale Numerical Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 6: Encode Labels with Combined Categories\n",
    "# Combine `y_train` and `y_test` to fit LabelEncoder on all possible labels\n",
    "all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Print the unique classes in the label\n",
    "print(\"Label classes:\", label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d63b011-9ec9-4c10-a4d0-b5ccda749bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 30, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Train the Random Forest Model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],      # Number of trees in the forest\n",
    "    'max_depth': [10, 20, 30, None],           # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],           # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],             # Minimum samples required to be at a leaf node\n",
    "    'class_weight': ['balanced', 'balanced_subsample']  # Handling imbalance\n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,                 # Number of parameter combinations to try\n",
    "    cv=3,                      # Number of cross-validation folds\n",
    "    verbose=2,                 # Level of logging\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # Use all available cores\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search on the training data\n",
    "rf_random.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Retrieve the best model after search\n",
    "best_rf_model = rf_random.best_estimator_\n",
    "print(\"Best Parameters:\", rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22e09980-568c-46d3-a061-dad27f60b8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        apache2       1.00      0.00      0.00       737\n",
      "           back       1.00      0.00      0.00       359\n",
      "buffer_overflow       1.00      0.00      0.00        20\n",
      "      ftp_write       1.00      0.00      0.00         3\n",
      "   guess_passwd       1.00      0.00      0.00      1231\n",
      "     httptunnel       1.00      0.00      0.00       133\n",
      "           imap       1.00      0.00      0.00         1\n",
      "        ipsweep       1.00      0.00      0.00       141\n",
      "           land       0.00      0.00      0.00         7\n",
      "     loadmodule       1.00      0.00      0.00         2\n",
      "       mailbomb       1.00      0.00      0.00       293\n",
      "          mscan       1.00      0.00      0.00       996\n",
      "       multihop       1.00      0.00      0.00        18\n",
      "          named       1.00      0.00      0.00        17\n",
      "        neptune       0.72      0.32      0.44      4656\n",
      "           nmap       1.00      0.00      0.00        73\n",
      "         normal       0.60      0.93      0.73      9711\n",
      "           perl       1.00      0.00      0.00         2\n",
      "            phf       1.00      0.00      0.00         2\n",
      "            pod       1.00      0.00      0.00        41\n",
      "      portsweep       0.04      0.99      0.07       157\n",
      "   processtable       1.00      0.00      0.00       685\n",
      "             ps       1.00      0.00      0.00        15\n",
      "        rootkit       1.00      0.00      0.00        13\n",
      "          saint       1.00      0.00      0.00       319\n",
      "          satan       0.14      0.04      0.06       735\n",
      "       sendmail       1.00      0.00      0.00        14\n",
      "          smurf       1.00      0.00      0.00       665\n",
      "  snmpgetattack       1.00      0.00      0.00       178\n",
      "      snmpguess       1.00      0.00      0.00       331\n",
      "      sqlattack       1.00      0.00      0.00         2\n",
      "       teardrop       1.00      0.00      0.00        12\n",
      "       udpstorm       1.00      0.00      0.00         2\n",
      "    warezmaster       1.00      0.00      0.00       944\n",
      "           worm       1.00      0.00      0.00         2\n",
      "          xlock       1.00      0.00      0.00         9\n",
      "         xsnoop       1.00      0.00      0.00         4\n",
      "          xterm       1.00      0.00      0.00        13\n",
      "\n",
      "       accuracy                           0.48     22543\n",
      "      macro avg       0.91      0.06      0.03     22543\n",
      "   weighted avg       0.74      0.48      0.41     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Train the Random Forest Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced',\n",
    "   random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train_encoded)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Decode predictions back to original labels for readability\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test_encoded)\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_labels, y_pred_labels))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54659ed3-3074-440d-81d2-d05ef82f8e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix After Targeted Attack:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Classification Report After Targeted Attack:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        apache2       1.00      0.00      0.00       737\n",
      "           back       1.00      0.00      0.00       359\n",
      "buffer_overflow       1.00      0.00      0.00        20\n",
      "      ftp_write       1.00      0.00      0.00         3\n",
      "   guess_passwd       1.00      0.00      0.00      1231\n",
      "     httptunnel       1.00      0.00      0.00       133\n",
      "           imap       1.00      0.00      0.00         1\n",
      "        ipsweep       0.00      0.00      0.00       141\n",
      "           land       0.00      0.00      0.00         7\n",
      "     loadmodule       1.00      0.00      0.00         2\n",
      "       mailbomb       1.00      0.00      0.00       293\n",
      "          mscan       1.00      0.00      0.00       996\n",
      "       multihop       0.00      0.00      0.00        18\n",
      "          named       1.00      0.00      0.00        17\n",
      "        neptune       0.64      0.05      0.09      4656\n",
      "           nmap       0.00      0.00      0.00        73\n",
      "         normal       0.44      0.87      0.58      9711\n",
      "           perl       1.00      0.00      0.00         2\n",
      "            phf       1.00      0.00      0.00         2\n",
      "            pod       1.00      0.00      0.00        41\n",
      "      portsweep       0.01      0.03      0.02       157\n",
      "   processtable       1.00      0.00      0.00       685\n",
      "             ps       1.00      0.00      0.00        15\n",
      "        rootkit       0.00      0.00      0.00        13\n",
      "          saint       1.00      0.00      0.00       319\n",
      "          satan       0.01      0.00      0.00       735\n",
      "       sendmail       1.00      0.00      0.00        14\n",
      "          smurf       0.06      0.04      0.05       665\n",
      "  snmpgetattack       1.00      0.00      0.00       178\n",
      "      snmpguess       1.00      0.00      0.00       331\n",
      "      sqlattack       1.00      0.00      0.00         2\n",
      "       teardrop       1.00      0.00      0.00        12\n",
      "       udpstorm       1.00      0.00      0.00         2\n",
      "    warezclient       0.00      1.00      0.00         0\n",
      "    warezmaster       0.01      0.00      0.00       944\n",
      "           worm       1.00      0.00      0.00         2\n",
      "          xlock       1.00      0.00      0.00         9\n",
      "         xsnoop       1.00      0.00      0.00         4\n",
      "          xterm       1.00      0.00      0.00        13\n",
      "\n",
      "       accuracy                           0.39     22543\n",
      "      macro avg       0.72      0.05      0.02     22543\n",
      "   weighted avg       0.56      0.39      0.27     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Extract critical feature indices manually from the screenshots\n",
    "critical_features = [2, 1, 8, 23, 20, 26]  # Replace these indices with those from your SHAP analysis.\n",
    "\n",
    "# Step 2: Implement a targeted attack\n",
    "def targeted_attack(X, critical_features, epsilon=3.0):\n",
    "    \"\"\"\n",
    "    Apply a targeted perturbation attack on the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    X (array): The input dataset.\n",
    "    critical_features (list): Indices of critical features to perturb.\n",
    "    epsilon (float): The perturbation amount.\n",
    "\n",
    "    Returns:\n",
    "    X_perturbed (array): The perturbed dataset.\n",
    "    \"\"\"\n",
    "    X_perturbed = X.copy()\n",
    "    for feature_idx in critical_features:\n",
    "        perturbation = np.random.uniform(-epsilon, epsilon, size=X_perturbed.shape[0])\n",
    "        X_perturbed[:, feature_idx] += perturbation\n",
    "    return X_perturbed\n",
    "\n",
    "# Step 3: Apply the attack\n",
    "X_test_perturbed = targeted_attack(X_test_scaled, critical_features, epsilon=3.0)\n",
    "\n",
    "# Step 4: Evaluate the model on the perturbed dataset\n",
    "y_pred_perturbed = rf_model.predict(X_test_perturbed)\n",
    "\n",
    "# Decode predictions back to original labels for readability\n",
    "y_pred_perturbed_labels = label_encoder.inverse_transform(y_pred_perturbed)\n",
    "\n",
    "# Step 5: Generate evaluation metrics\n",
    "print(\"Confusion Matrix After Targeted Attack:\")\n",
    "print(confusion_matrix(y_test_labels, y_pred_perturbed_labels))\n",
    "\n",
    "print(\"\\nClassification Report After Targeted Attack:\")\n",
    "print(classification_report(y_test_labels, y_pred_perturbed_labels, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba49c10-2736-4236-a165-9456978a2119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/art/estimators/certification/__init__.py:30: UserWarning: PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\n",
      "  warnings.warn(\"PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from art.estimators.classification import SklearnClassifier\n",
    "from art.attacks.evasion import BoundaryAttack\n",
    "from art.attacks.evasion import HopSkipJump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95a2f141-7ea2-4b5b-b974-47ed42a3442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34649336822960564\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        apache2       1.00      0.00      0.00       737\n",
      "           back       1.00      0.00      0.00       359\n",
      "buffer_overflow       1.00      0.00      0.00        20\n",
      "      ftp_write       1.00      0.00      0.00         3\n",
      "   guess_passwd       0.00      0.00      0.00      1231\n",
      "     httptunnel       1.00      0.00      0.00       133\n",
      "           imap       1.00      0.00      0.00         1\n",
      "        ipsweep       1.00      0.00      0.00       141\n",
      "           land       0.00      0.00      0.00         7\n",
      "     loadmodule       1.00      0.00      0.00         2\n",
      "       mailbomb       1.00      0.00      0.00       293\n",
      "          mscan       1.00      0.00      0.00       996\n",
      "       multihop       1.00      0.00      0.00        18\n",
      "          named       1.00      0.00      0.00        17\n",
      "        neptune       0.69      0.32      0.44      4656\n",
      "           nmap       1.00      0.00      0.00        73\n",
      "         normal       0.54      0.61      0.57      9711\n",
      "           perl       1.00      0.00      0.00         2\n",
      "            phf       1.00      0.00      0.00         2\n",
      "            pod       1.00      0.00      0.00        41\n",
      "      portsweep       0.03      0.84      0.06       157\n",
      "   processtable       1.00      0.00      0.00       685\n",
      "             ps       1.00      0.00      0.00        15\n",
      "        rootkit       0.00      0.00      0.00        13\n",
      "          saint       1.00      0.00      0.00       319\n",
      "          satan       0.60      0.39      0.48       735\n",
      "       sendmail       1.00      0.00      0.00        14\n",
      "          smurf       1.00      0.00      0.00       665\n",
      "  snmpgetattack       1.00      0.00      0.00       178\n",
      "      snmpguess       1.00      0.00      0.00       331\n",
      "      sqlattack       1.00      0.00      0.00         2\n",
      "       teardrop       1.00      0.00      0.00        12\n",
      "       udpstorm       1.00      0.00      0.00         2\n",
      "    warezmaster       1.00      0.00      0.00       944\n",
      "           worm       1.00      0.00      0.00         2\n",
      "          xlock       1.00      0.00      0.00         9\n",
      "         xsnoop       1.00      0.00      0.00         4\n",
      "          xterm       1.00      0.00      0.00        13\n",
      "\n",
      "       accuracy                           0.35     22543\n",
      "      macro avg       0.86      0.06      0.04     22543\n",
      "   weighted avg       0.66      0.35      0.35     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Step 1: Create XGBoost DMatrix for training and testing\n",
    "# XGBoost requires data to be converted into DMatrix format for efficient training\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train_encoded)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test_encoded)\n",
    "\n",
    "# Step 2: Set up parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': len(label_encoder.classes_),  # number of classes\n",
    "    'eval_metric': 'mlogloss',  # evaluation metric\n",
    "    'max_depth': 6,  # maximum depth of the tree\n",
    "    'eta': 0.1,  # learning rate\n",
    "    'subsample': 0.8,  # fraction of samples to use for each tree\n",
    "    'colsample_bytree': 0.8,  # fraction of features to use for each tree\n",
    "    'seed': 42  # for reproducibility\n",
    "}\n",
    "\n",
    "# Step 3: Train the model\n",
    "num_round = 100  # number of training rounds\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "import numpy as np\n",
    "# Ensure predictions are integers within the valid range\n",
    "y_pred = y_pred.astype(int)  # Ensure predictions are integer values\n",
    "\n",
    "# Filter `unique_labels` to contain only valid indices within the range of `label_encoder.classes_`\n",
    "unique_labels = np.unique(np.concatenate([y_test_encoded, y_pred]))\n",
    "unique_labels = unique_labels[unique_labels < len(label_encoder.classes_)]\n",
    "\n",
    "# Get filtered target names based on `unique_labels`\n",
    "filtered_target_names = [label_encoder.classes_[i] for i in unique_labels]\n",
    "\n",
    "# Print the classification report with the filtered target names\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_encoded, y_pred, labels=unique_labels, target_names=filtered_target_names,zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50ef1a5e-3260-4ef2-90f8-474991b06070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy After Stronger Attack: 0.16253382424699464\n",
      "Classification Report After Stronger Attack:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        apache2       1.00      0.00      0.00       737\n",
      "           back       1.00      0.00      0.00       359\n",
      "buffer_overflow       1.00      0.00      0.00        20\n",
      "      ftp_write       1.00      0.00      0.00         3\n",
      "   guess_passwd       0.00      0.00      0.00      1231\n",
      "     httptunnel       1.00      0.00      0.00       133\n",
      "           imap       0.00      0.00      0.00         1\n",
      "        ipsweep       0.00      0.00      0.00       141\n",
      "           land       0.00      0.00      0.00         7\n",
      "     loadmodule       0.00      0.00      0.00         2\n",
      "       mailbomb       1.00      0.00      0.00       293\n",
      "          mscan       1.00      0.00      0.00       996\n",
      "       multihop       1.00      0.00      0.00        18\n",
      "          named       1.00      0.00      0.00        17\n",
      "        neptune       0.73      0.09      0.17      4656\n",
      "           nmap       0.00      0.00      0.00        73\n",
      "         normal       0.31      0.32      0.32      9711\n",
      "           perl       1.00      0.00      0.00         2\n",
      "            phf       1.00      0.00      0.00         2\n",
      "            pod       0.00      0.20      0.01        41\n",
      "      portsweep       0.01      0.01      0.01       157\n",
      "   processtable       1.00      0.00      0.00       685\n",
      "             ps       1.00      0.00      0.00        15\n",
      "        rootkit       1.00      0.00      0.00        13\n",
      "          saint       1.00      0.00      0.00       319\n",
      "          satan       0.03      0.08      0.05       735\n",
      "       sendmail       1.00      0.00      0.00        14\n",
      "          smurf       0.00      0.00      0.00       665\n",
      "  snmpgetattack       1.00      0.00      0.00       178\n",
      "      snmpguess       1.00      0.00      0.00       331\n",
      "      sqlattack       1.00      0.00      0.00         2\n",
      "       teardrop       0.00      0.08      0.00        12\n",
      "       udpstorm       1.00      0.00      0.00         2\n",
      "    warezclient       0.00      1.00      0.00         0\n",
      "    warezmaster       0.04      0.01      0.02       944\n",
      "           worm       1.00      0.00      0.00         2\n",
      "          xlock       1.00      0.00      0.00         9\n",
      "         xsnoop       1.00      0.00      0.00         4\n",
      "          xterm       1.00      0.00      0.00        13\n",
      "\n",
      "       accuracy                           0.16     22543\n",
      "      macro avg       0.64      0.05      0.01     22543\n",
      "   weighted avg       0.47      0.16      0.17     22543\n",
      "\n",
      "Confusion Matrix After Stronger Targeted Attack:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "critical_features = [2, 1, 8, 3, 24, 26]\n",
    "# Amplified perturbation with a selective feature targeting mechanism\n",
    "def stronger_perturb_features(X, feature_indices, noise_factor=0.5, target_fraction=0.5):\n",
    "    \"\"\"\n",
    "    Apply amplified random noise to a subset of critical features for a stronger attack.\n",
    "    Args:\n",
    "        X: The feature matrix (numpy array).\n",
    "        feature_indices: List of indices for critical features to perturb.\n",
    "        noise_factor: The scale of the noise to apply.\n",
    "        target_fraction: Fraction of samples to perturb.\n",
    "    Returns:\n",
    "        X_perturbed: The perturbed feature matrix.\n",
    "    \"\"\"\n",
    "    X_perturbed = X.copy()\n",
    "    num_samples = X.shape[0]\n",
    "    # Determine the samples to attack (randomly select a fraction)\n",
    "    attack_samples = np.random.choice(\n",
    "        num_samples, size=int(num_samples * target_fraction), replace=False\n",
    "    )\n",
    "    \n",
    "    for idx in feature_indices:\n",
    "        # Apply stronger noise only to the selected samples\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=len(attack_samples))\n",
    "        X_perturbed[attack_samples, idx] += noise\n",
    "    return X_perturbed\n",
    "\n",
    "# Apply a stronger perturbation to the test set\n",
    "X_test_stronger_perturbed = stronger_perturb_features(\n",
    "    X_test_scaled, critical_feature_indices, noise_factor=5.5, target_fraction=1.0\n",
    ")\n",
    "\n",
    "# Predict on the stronger perturbed test set using the XGBoost model\n",
    "dtest_stronger_perturbed = xgb.DMatrix(X_test_stronger_perturbed)\n",
    "y_pred_stronger_perturbed = bst.predict(dtest_stronger_perturbed)\n",
    "\n",
    "# Ensure predictions are integers within the valid range\n",
    "y_pred_stronger_perturbed = y_pred_stronger_perturbed.astype(int)\n",
    "\n",
    "# Filter `unique_labels` to contain only valid indices within the range of `label_encoder.classes_`\n",
    "unique_labels = np.unique(np.concatenate([y_test_encoded, y_pred_stronger_perturbed]))\n",
    "unique_labels = unique_labels[unique_labels < len(label_encoder.classes_)]\n",
    "\n",
    "# Get filtered target names based on `unique_labels`\n",
    "filtered_target_names = [label_encoder.classes_[i] for i in unique_labels]\n",
    "\n",
    "# Evaluate the model's performance after the stronger attack\n",
    "print(\"Accuracy After Stronger Attack:\", accuracy_score(y_test_encoded, y_pred_stronger_perturbed))\n",
    "print(\"Classification Report After Stronger Attack:\\n\", classification_report(\n",
    "    y_test_encoded, y_pred_stronger_perturbed, labels=unique_labels, target_names=filtered_target_names, zero_division=1))\n",
    "print(\"Confusion Matrix After Stronger Targeted Attack:\\n\", confusion_matrix(y_test_encoded, y_pred_stronger_perturbed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0700fb-7256-4090-88ee-851c9be4df8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
