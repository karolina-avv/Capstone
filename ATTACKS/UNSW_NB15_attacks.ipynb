{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "83e78939-d3ea-444f-8148-580a81f6ee46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6216e408-ff98-4d09-864b-29d7adaa804b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/Users/marlenawasiak/Desktop/Data_Collection/UNSW_NB15_training-set.csv')\n",
    "test_data = pd.read_csv('/Users/marlenawasiak/Desktop/Data_Collection/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2eee4bc3-7d5f-44cc-8a57-7e0554a49d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['attack_cat'])\n",
    "y_train = train_data['attack_cat']\n",
    "\n",
    "X_test = test_data.drop(columns=['attack_cat'])\n",
    "y_test = test_data['attack_cat']\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_features = ['proto', 'service', 'state']\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Align test set columns with training set columns (fill missing columns with 0)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Encode the target label (attack category) in both training and test sets\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Apply oversampling to the training set\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the training and test sets\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)  # Fit and transform on training set\n",
    "X_test_scaled = scaler.transform(X_test)                     # Only transform the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aedbdb1d-c373-4138-9301-556f7de8f457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 592us/step - accuracy: 0.6430 - loss: 0.8994 - val_accuracy: 0.6394 - val_loss: 1.3868\n",
      "Epoch 2/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 622us/step - accuracy: 0.7240 - loss: 0.6669 - val_accuracy: 0.6317 - val_loss: 2.1254\n",
      "Epoch 3/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 635us/step - accuracy: 0.7337 - loss: 0.6376 - val_accuracy: 0.7726 - val_loss: 1.6026\n",
      "Epoch 4/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 601us/step - accuracy: 0.7381 - loss: 0.6215 - val_accuracy: 0.7698 - val_loss: 1.8644\n",
      "Epoch 5/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 652us/step - accuracy: 0.7421 - loss: 0.6072 - val_accuracy: 0.7093 - val_loss: 2.5264\n",
      "Epoch 6/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 644us/step - accuracy: 0.7455 - loss: 0.5999 - val_accuracy: 0.7764 - val_loss: 2.3490\n",
      "Epoch 7/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 639us/step - accuracy: 0.7471 - loss: 0.5928 - val_accuracy: 0.7909 - val_loss: 2.7585\n",
      "Epoch 8/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 654us/step - accuracy: 0.7501 - loss: 0.5836 - val_accuracy: 0.7862 - val_loss: 2.9562\n",
      "Epoch 9/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 598us/step - accuracy: 0.7516 - loss: 0.5794 - val_accuracy: 0.7574 - val_loss: 3.5629\n",
      "Epoch 10/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 616us/step - accuracy: 0.7518 - loss: 0.5781 - val_accuracy: 0.7620 - val_loss: 3.5647\n",
      "Epoch 11/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 653us/step - accuracy: 0.7525 - loss: 0.5729 - val_accuracy: 0.7824 - val_loss: 3.3734\n",
      "Epoch 12/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 608us/step - accuracy: 0.7546 - loss: 0.5711 - val_accuracy: 0.7664 - val_loss: 3.6899\n",
      "Epoch 13/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 628us/step - accuracy: 0.7565 - loss: 0.5649 - val_accuracy: 0.7837 - val_loss: 3.5239\n",
      "Epoch 14/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 645us/step - accuracy: 0.7558 - loss: 0.5659 - val_accuracy: 0.7238 - val_loss: 5.0181\n",
      "Epoch 15/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 600us/step - accuracy: 0.7580 - loss: 0.5595 - val_accuracy: 0.7605 - val_loss: 4.0745\n",
      "Epoch 16/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 706us/step - accuracy: 0.7577 - loss: 0.5594 - val_accuracy: 0.7710 - val_loss: 4.0823\n",
      "Epoch 17/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 580us/step - accuracy: 0.7593 - loss: 0.5585 - val_accuracy: 0.7580 - val_loss: 4.3284\n",
      "Epoch 18/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 613us/step - accuracy: 0.7593 - loss: 0.5552 - val_accuracy: 0.7504 - val_loss: 4.1588\n",
      "Epoch 19/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 532us/step - accuracy: 0.7591 - loss: 0.5540 - val_accuracy: 0.7488 - val_loss: 5.0879\n",
      "Epoch 20/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 523us/step - accuracy: 0.7603 - loss: 0.5514 - val_accuracy: 0.6712 - val_loss: 5.7274\n",
      "Epoch 21/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 588us/step - accuracy: 0.7611 - loss: 0.5483 - val_accuracy: 0.7199 - val_loss: 5.4443\n",
      "Epoch 22/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 594us/step - accuracy: 0.7622 - loss: 0.5487 - val_accuracy: 0.6294 - val_loss: 7.7864\n",
      "Epoch 23/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 653us/step - accuracy: 0.7615 - loss: 0.5494 - val_accuracy: 0.7221 - val_loss: 5.5239\n",
      "Epoch 24/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 603us/step - accuracy: 0.7616 - loss: 0.5492 - val_accuracy: 0.7344 - val_loss: 5.6475\n",
      "Epoch 25/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 595us/step - accuracy: 0.7631 - loss: 0.5461 - val_accuracy: 0.7060 - val_loss: 5.3209\n",
      "Epoch 26/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 608us/step - accuracy: 0.7637 - loss: 0.5439 - val_accuracy: 0.7350 - val_loss: 5.4492\n",
      "Epoch 27/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 613us/step - accuracy: 0.7641 - loss: 0.5437 - val_accuracy: 0.7604 - val_loss: 5.5433\n",
      "Epoch 28/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 598us/step - accuracy: 0.7644 - loss: 0.5427 - val_accuracy: 0.7196 - val_loss: 5.8874\n",
      "Epoch 29/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 612us/step - accuracy: 0.7649 - loss: 0.5413 - val_accuracy: 0.7255 - val_loss: 5.6579\n",
      "Epoch 30/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 580us/step - accuracy: 0.7654 - loss: 0.5415 - val_accuracy: 0.7646 - val_loss: 4.7990\n",
      "Epoch 31/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 589us/step - accuracy: 0.7644 - loss: 0.5411 - val_accuracy: 0.6621 - val_loss: 7.8823\n",
      "Epoch 32/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 599us/step - accuracy: 0.7644 - loss: 0.5402 - val_accuracy: 0.6460 - val_loss: 10.7147\n",
      "Epoch 33/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 583us/step - accuracy: 0.7652 - loss: 0.5392 - val_accuracy: 0.6255 - val_loss: 12.3863\n",
      "Epoch 34/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 680us/step - accuracy: 0.7673 - loss: 0.5409 - val_accuracy: 0.7213 - val_loss: 6.8794\n",
      "Epoch 35/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 622us/step - accuracy: 0.7666 - loss: 0.5371 - val_accuracy: 0.6737 - val_loss: 8.7029\n",
      "Epoch 36/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 623us/step - accuracy: 0.7677 - loss: 0.5352 - val_accuracy: 0.6594 - val_loss: 9.1307\n",
      "Epoch 37/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 607us/step - accuracy: 0.7678 - loss: 0.5336 - val_accuracy: 0.6915 - val_loss: 7.2052\n",
      "Epoch 38/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 553us/step - accuracy: 0.7683 - loss: 0.5354 - val_accuracy: 0.6783 - val_loss: 8.6852\n",
      "Epoch 39/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 587us/step - accuracy: 0.7681 - loss: 0.5339 - val_accuracy: 0.6568 - val_loss: 8.0603\n",
      "Epoch 40/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 593us/step - accuracy: 0.7683 - loss: 0.5314 - val_accuracy: 0.6791 - val_loss: 9.4660\n",
      "Epoch 41/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 593us/step - accuracy: 0.7693 - loss: 0.5315 - val_accuracy: 0.6267 - val_loss: 12.1726\n",
      "Epoch 42/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 625us/step - accuracy: 0.7698 - loss: 0.5312 - val_accuracy: 0.6388 - val_loss: 9.0162\n",
      "Epoch 43/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 587us/step - accuracy: 0.7686 - loss: 0.5363 - val_accuracy: 0.6586 - val_loss: 9.0969\n",
      "Epoch 44/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 602us/step - accuracy: 0.7701 - loss: 0.5320 - val_accuracy: 0.6320 - val_loss: 9.8401\n",
      "Epoch 45/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 611us/step - accuracy: 0.7692 - loss: 0.5363 - val_accuracy: 0.6478 - val_loss: 9.6130\n",
      "Epoch 46/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 609us/step - accuracy: 0.7706 - loss: 0.5318 - val_accuracy: 0.6284 - val_loss: 10.1135\n",
      "Epoch 47/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 640us/step - accuracy: 0.7697 - loss: 0.5328 - val_accuracy: 0.6176 - val_loss: 15.6436\n",
      "Epoch 48/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 751us/step - accuracy: 0.7705 - loss: 0.5294 - val_accuracy: 0.6348 - val_loss: 10.8301\n",
      "Epoch 49/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 737us/step - accuracy: 0.7700 - loss: 0.5332 - val_accuracy: 0.6542 - val_loss: 9.5570\n",
      "Epoch 50/50\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 668us/step - accuracy: 0.7710 - loss: 0.5288 - val_accuracy: 0.6789 - val_loss: 8.5386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Define the neural network model\n",
    "# Experiment with a deeper model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_resampled.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_resampled, y_train_resampled, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test), class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2c6ad789-3961-48bc-9e5c-4c5cf5b6b25a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301us/step\n",
      "Test Accuracy: 0.6788976339697809\n",
      "Classification Report - Clean Data:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.06      0.21      0.09       677\n",
      "      Backdoor       0.05      0.55      0.09       583\n",
      "           DoS       0.25      0.11      0.15      4089\n",
      "      Exploits       0.35      0.50      0.41     11132\n",
      "       Fuzzers       0.49      0.64      0.55      6062\n",
      "       Generic       1.00      0.33      0.50     18871\n",
      "        Normal       1.00      1.00      1.00     37000\n",
      "Reconnaissance       0.58      0.62      0.60      3496\n",
      "     Shellcode       0.20      0.43      0.27       378\n",
      "         Worms       0.12      0.20      0.15        44\n",
      "\n",
      "      accuracy                           0.68     82332\n",
      "     macro avg       0.41      0.46      0.38     82332\n",
      "  weighted avg       0.80      0.68      0.70     82332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred_encoded = model.predict(X_test_scaled).argmax(axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred_encoded)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Generate classification report with specified labels to handle all classes\n",
    "class_report = classification_report(\n",
    "    y_test, \n",
    "    y_pred_encoded, \n",
    "    labels=range(len(label_encoder.classes_)), \n",
    "    target_names=label_encoder.classes_, \n",
    "    zero_division=1\n",
    ")\n",
    "print(\"Classification Report - Clean Data:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4f701594-1bcf-426b-9a64-f175bea53b3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FGSM Attack ---\n",
      "\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step\n",
      "FGSM Adversarial Accuracy: 0.50010931351115\n",
      "Classification Report on FGSM Adversarial Examples:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.02      0.00      0.01       677\n",
      "      Backdoor       0.04      0.30      0.08       583\n",
      "           DoS       0.05      0.07      0.05      4089\n",
      "      Exploits       0.40      0.19      0.25     11132\n",
      "       Fuzzers       0.11      0.14      0.12      6062\n",
      "       Generic       0.73      0.03      0.07     18871\n",
      "        Normal       0.74      1.00      0.85     37000\n",
      "Reconnaissance       0.02      0.01      0.02      3496\n",
      "     Shellcode       0.02      0.30      0.04       378\n",
      "         Worms       0.00      0.00      0.00        44\n",
      "\n",
      "      accuracy                           0.50     82332\n",
      "     macro avg       0.21      0.20      0.15     82332\n",
      "  weighted avg       0.56      0.50      0.44     82332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, CarliniL2Method\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Critical features from SHAP analysis\n",
    "critical_features = [40, 174, 159, 31, 25]\n",
    "\n",
    "# Create a mask to focus on the critical features\n",
    "mask = np.zeros(X_train_resampled.shape[1], dtype=np.float32)\n",
    "mask[critical_features] = 1.0\n",
    "\n",
    "# Define the ART classifier\n",
    "classifier = TensorFlowV2Classifier(\n",
    "    model=model,\n",
    "    nb_classes=len(label_encoder.classes_),\n",
    "    input_shape=(X_train_resampled.shape[1],),\n",
    "    loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    ")\n",
    "\n",
    "# Apply FGSM attack\n",
    "print(\"\\n--- FGSM Attack ---\")\n",
    "fgsm = FastGradientMethod(estimator=classifier, eps=3.0, targeted=False)\n",
    "X_test_adv_fgsm = fgsm.generate(X_test_scaled, mask=mask)\n",
    "fgsm_preds = model.predict(X_test_adv_fgsm).argmax(axis=1)\n",
    "fgsm_accuracy = accuracy_score(y_test, fgsm_preds)\n",
    "fgsm_class_report = classification_report(y_test, fgsm_preds, target_names=label_encoder.classes_, zero_division=1)\n",
    "print(f\"FGSM Adversarial Accuracy: {fgsm_accuracy}\")\n",
    "print(f\"Classification Report on FGSM Adversarial Examples:\\n{fgsm_class_report}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7c437d10-f2bb-4083-b621-1bd71f9d2a35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PGD Attack ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 23:54:09.603669: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258us/step\n",
      "PGD Adversarial Accuracy: 0.5566608366127387\n",
      "Classification Report on PGD Adversarial Examples:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.01      0.06      0.02       677\n",
      "      Backdoor       0.04      0.54      0.07       583\n",
      "           DoS       0.07      0.10      0.09      4089\n",
      "      Exploits       0.35      0.16      0.22     11132\n",
      "       Fuzzers       0.12      0.13      0.12      6062\n",
      "       Generic       0.95      0.27      0.42     18871\n",
      "        Normal       0.89      1.00      0.94     37000\n",
      "Reconnaissance       0.13      0.13      0.13      3496\n",
      "     Shellcode       0.02      0.16      0.03       378\n",
      "         Worms       0.02      0.11      0.03        44\n",
      "\n",
      "      accuracy                           0.56     82332\n",
      "     macro avg       0.26      0.27      0.21     82332\n",
      "  weighted avg       0.68      0.56      0.57     82332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply PGD attack with dynamic feature masking\n",
    "print(\"\\n--- PGD Attack ---\")\n",
    "pgd = ProjectedGradientDescent(estimator=classifier, eps=2.0, eps_step=0.1, max_iter=100, targeted=False)\n",
    "X_test_adv_pgd = pgd.generate(X_test_scaled, mask=mask)\n",
    "pgd_preds = model.predict(X_test_adv_pgd).argmax(axis=1)\n",
    "pgd_accuracy = accuracy_score(y_test, pgd_preds)\n",
    "pgd_class_report = classification_report(y_test, pgd_preds, target_names=label_encoder.classes_, zero_division=1)\n",
    "print(f\"PGD Adversarial Accuracy: {pgd_accuracy}\")\n",
    "print(f\"Classification Report on PGD Adversarial Examples:\\n{pgd_class_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "de5bfbe6-53aa-43dd-b38f-5c8015d8e89c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Carlini & Wagner Attack (on a subset) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e5a7dd86d94b4294f88f292c073ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step\n",
      "Carlini & Wagner Adversarial Accuracy (Subset): 0.20850202429149797\n",
      "\n",
      "Classification Report on Carlini & Wagner Adversarial Examples (Subset):\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.00      0.00      0.00        50\n",
      "      Backdoor       0.15      0.92      0.26        50\n",
      "           DoS       0.00      0.00      0.00        50\n",
      "      Exploits       0.09      0.02      0.03        50\n",
      "       Fuzzers       0.41      0.18      0.25        50\n",
      "       Generic       0.00      0.00      0.00        50\n",
      "        Normal       0.31      0.92      0.46        50\n",
      "Reconnaissance       0.00      0.00      0.00        50\n",
      "     Shellcode       1.00      0.00      0.00        50\n",
      "         Worms       1.00      0.02      0.04        44\n",
      "\n",
      "      accuracy                           0.21       494\n",
      "     macro avg       0.30      0.21      0.11       494\n",
      "  weighted avg       0.29      0.21      0.11       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Carlini & Wagner Attack (on a subset) ---\")\n",
    "\n",
    "# Select a subset that includes multiple classes\n",
    "subset_indices = []\n",
    "classes_to_include = np.unique(y_test)\n",
    "for cls in classes_to_include:\n",
    "    indices = np.where(y_test == cls)[0][:50]  # Take up to 50 samples per class\n",
    "    subset_indices.extend(indices)\n",
    "\n",
    "# Create a subset based on the selected indices\n",
    "subset_indices = np.array(subset_indices)\n",
    "X_test_subset = X_test_scaled[subset_indices]\n",
    "y_test_subset = y_test_encoded[subset_indices]\n",
    "\n",
    "# Run Carlini & Wagner attack on the subset\n",
    "cw = CarliniL2Method(classifier=classifier, confidence=1.0, targeted=False, max_iter=100)\n",
    "X_test_adv_cw_subset = cw.generate(X_test_subset, mask=mask)\n",
    "\n",
    "# Evaluate the attack\n",
    "cw_preds_subset = mlp_model.predict(X_test_adv_cw_subset).argmax(axis=1)\n",
    "cw_accuracy_subset = accuracy_score(y_test_subset, cw_preds_subset)\n",
    "cw_class_report_subset = classification_report(\n",
    "    y_test_subset, \n",
    "    cw_preds_subset, \n",
    "    target_names=label_encoder.classes_, \n",
    "    zero_division=1\n",
    ")\n",
    "\n",
    "print(f\"Carlini & Wagner Adversarial Accuracy (Subset): {cw_accuracy_subset}\")\n",
    "print(f\"\\nClassification Report on Carlini & Wagner Adversarial Examples (Subset):\\n{cw_class_report_subset}\")\n",
    "results = {\n",
    "    \"clean\": {\n",
    "        \"accuracy\": clean_accuracy,\n",
    "        \"report\": clean_class_report\n",
    "    },\n",
    "    \"fgsm\": {\n",
    "        \"accuracy\": fgsm_accuracy,\n",
    "        \"report\": fgsm_class_report\n",
    "    },\n",
    "    \"pgd\": {\n",
    "        \"accuracy\": pgd_accuracy,\n",
    "        \"report\": pgd_class_report\n",
    "}\n",
    "}\n",
    "# Save results for C&W attack on the subset\n",
    "results[\"cw_subset\"] = {\n",
    "    \"accuracy\": cw_accuracy_subset,\n",
    "    \"report\": cw_class_report_subset\n",
    "}\n",
    "\n",
    "np.save(\"dynamic_feature_mask.npy\", mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "63300885-eab6-4535-8180-f8446b6aa4cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 497us/step - accuracy: 0.6358 - loss: 0.9265 - val_accuracy: 0.8372 - val_loss: 0.4638\n",
      "Epoch 2/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 492us/step - accuracy: 0.7053 - loss: 0.7201 - val_accuracy: 0.8332 - val_loss: 0.4546\n",
      "Epoch 3/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 528us/step - accuracy: 0.7154 - loss: 0.6920 - val_accuracy: 0.8371 - val_loss: 0.4342\n",
      "Epoch 4/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 471us/step - accuracy: 0.7206 - loss: 0.6757 - val_accuracy: 0.7945 - val_loss: 0.6712\n",
      "Epoch 5/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 466us/step - accuracy: 0.7279 - loss: 0.6601 - val_accuracy: 0.8250 - val_loss: 0.9787\n",
      "Epoch 6/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 496us/step - accuracy: 0.7310 - loss: 0.6489 - val_accuracy: 0.8391 - val_loss: 0.5680\n",
      "Epoch 7/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 502us/step - accuracy: 0.7290 - loss: 0.6483 - val_accuracy: 0.8346 - val_loss: 0.8540\n",
      "Epoch 8/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 483us/step - accuracy: 0.7344 - loss: 0.6401 - val_accuracy: 0.8127 - val_loss: 1.1723\n",
      "Epoch 9/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 499us/step - accuracy: 0.7369 - loss: 0.6317 - val_accuracy: 0.8355 - val_loss: 1.1104\n",
      "Epoch 10/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 491us/step - accuracy: 0.7390 - loss: 0.6255 - val_accuracy: 0.8277 - val_loss: 1.0015\n",
      "Epoch 11/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 478us/step - accuracy: 0.7375 - loss: 0.6291 - val_accuracy: 0.8359 - val_loss: 1.2283\n",
      "Epoch 12/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 493us/step - accuracy: 0.7411 - loss: 0.6213 - val_accuracy: 0.8305 - val_loss: 1.2554\n",
      "Epoch 13/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 489us/step - accuracy: 0.7404 - loss: 0.6208 - val_accuracy: 0.8369 - val_loss: 1.2691\n",
      "Epoch 14/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 498us/step - accuracy: 0.7433 - loss: 0.6127 - val_accuracy: 0.8317 - val_loss: 1.0305\n",
      "Epoch 15/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 492us/step - accuracy: 0.7392 - loss: 0.6207 - val_accuracy: 0.8361 - val_loss: 1.2752\n",
      "Epoch 16/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 489us/step - accuracy: 0.7433 - loss: 0.6127 - val_accuracy: 0.8321 - val_loss: 1.2465\n",
      "Epoch 17/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 493us/step - accuracy: 0.7439 - loss: 0.6106 - val_accuracy: 0.8378 - val_loss: 1.1033\n",
      "Epoch 18/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 490us/step - accuracy: 0.7452 - loss: 0.6067 - val_accuracy: 0.8360 - val_loss: 0.9731\n",
      "Epoch 19/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 479us/step - accuracy: 0.7454 - loss: 0.6068 - val_accuracy: 0.8362 - val_loss: 1.0931\n",
      "Epoch 20/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 512us/step - accuracy: 0.7457 - loss: 0.6072 - val_accuracy: 0.8378 - val_loss: 1.3625\n",
      "Epoch 21/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 486us/step - accuracy: 0.7446 - loss: 0.6078 - val_accuracy: 0.8391 - val_loss: 1.0480\n",
      "Epoch 22/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 482us/step - accuracy: 0.7444 - loss: 0.6076 - val_accuracy: 0.8312 - val_loss: 0.9419\n",
      "Epoch 23/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 509us/step - accuracy: 0.7433 - loss: 0.6081 - val_accuracy: 0.8354 - val_loss: 1.1656\n",
      "Epoch 24/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 510us/step - accuracy: 0.7450 - loss: 0.6094 - val_accuracy: 0.8290 - val_loss: 1.2183\n",
      "Epoch 25/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 488us/step - accuracy: 0.7456 - loss: 0.6075 - val_accuracy: 0.8382 - val_loss: 1.2409\n",
      "Epoch 26/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 494us/step - accuracy: 0.7465 - loss: 0.6016 - val_accuracy: 0.8350 - val_loss: 1.1286\n",
      "Epoch 27/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 484us/step - accuracy: 0.7472 - loss: 0.6040 - val_accuracy: 0.8368 - val_loss: 0.8275\n",
      "Epoch 28/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 481us/step - accuracy: 0.7465 - loss: 0.6042 - val_accuracy: 0.8364 - val_loss: 1.2867\n",
      "Epoch 29/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 492us/step - accuracy: 0.7459 - loss: 0.6062 - val_accuracy: 0.8321 - val_loss: 1.4250\n",
      "Epoch 30/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 489us/step - accuracy: 0.7485 - loss: 0.5988 - val_accuracy: 0.8346 - val_loss: 1.0505\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# Assuming train_data and test_data are available\n",
    "X_train = train_data.drop(columns=['attack_cat'])\n",
    "y_train = train_data['attack_cat']\n",
    "X_test = test_data.drop(columns=['attack_cat'])\n",
    "y_test = test_data['attack_cat']\n",
    "\n",
    "# Step 1: One-hot encode categorical features\n",
    "categorical_features = ['proto', 'service', 'state']\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Step 2: Encode the target labels (attack category)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 3: Handle class imbalance with oversampling\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train_encoded)\n",
    "\n",
    "# Step 4: Scale features using StandardScaler and QuantileTransformer\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "X_train_scaled = quantile_transformer.fit_transform(X_train_scaled)\n",
    "X_test_scaled = quantile_transformer.transform(X_test_scaled)\n",
    "\n",
    "# Step 5: Define and compile the MLP model\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')  # Ensure the output layer matches the number of classes\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train_resampled, epochs=30, batch_size=32, validation_data=(X_test_scaled, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75a92e03-a1fa-447c-b813-037a32a57bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step\n",
      "MLP Model Accuracy on Clean Data: 0.8394427440120488\n",
      "\n",
      "Classification Report on Clean Data with Attack Names:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.00      0.00      0.00       677\n",
      "      Backdoor       0.06      0.96      0.12       583\n",
      "           DoS       0.33      0.07      0.11      4089\n",
      "      Exploits       0.85      0.52      0.64     11132\n",
      "       Fuzzers       0.86      0.68      0.76      6062\n",
      "       Generic       1.00      0.96      0.98     18871\n",
      "        Normal       1.00      1.00      1.00     37000\n",
      "Reconnaissance       0.84      0.85      0.84      3496\n",
      "     Shellcode       0.21      0.87      0.34       378\n",
      "         Worms       0.08      0.77      0.15        44\n",
      "\n",
      "      accuracy                           0.84     82332\n",
      "     macro avg       0.52      0.67      0.49     82332\n",
      "  weighted avg       0.91      0.84      0.86     82332\n",
      "\n",
      "\n",
      "Confusion Matrix on Clean Data with Attack Names:\n",
      " [[    0   618     8    50     1     0     0     0     0     0]\n",
      " [    1   557     1     8     3     0     0     2    10     1]\n",
      " [   50  2900   277   544   116     9     3    40   127    23]\n",
      " [  213  3215   362  5771   443     2    16   463   384   263]\n",
      " [    1  1300    20    55  4108     0     0    25   508    45]\n",
      " [    6    62   169   317    88 18082     0    33    94    20]\n",
      " [    0     0     0     0     0     0 37000     0     0     0]\n",
      " [    5   350     6    32    14     0     0  2955   116    18]\n",
      " [    0    10     1     2    18     0     0     8   329    10]\n",
      " [    0     0     0     4     1     0     0     0     5    34]]\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the model on clean data\n",
    "y_pred = mlp_model.predict(X_test_scaled).argmax(axis=1)\n",
    "clean_accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "\n",
    "# Step 8: Decode predictions for a report with original class labels\n",
    "y_test_labels = label_encoder.inverse_transform(y_test_encoded)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Classification report and confusion matrix with decoded labels\n",
    "decoded_class_report = classification_report(y_test_labels, y_pred_labels, zero_division=1)\n",
    "decoded_conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Print the results\n",
    "print(\"MLP Model Accuracy on Clean Data:\", clean_accuracy)\n",
    "print(\"\\nClassification Report on Clean Data with Attack Names:\\n\", decoded_class_report)\n",
    "print(\"\\nConfusion Matrix on Clean Data with Attack Names:\\n\", decoded_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "47226656-5438-4b8c-a736-015834e19f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 580us/step - accuracy: 0.6303 - loss: 0.9373 - val_accuracy: 0.8200 - val_loss: 0.5085\n",
      "Epoch 2/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 465us/step - accuracy: 0.7055 - loss: 0.7171 - val_accuracy: 0.8379 - val_loss: 0.4307\n",
      "Epoch 3/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 550us/step - accuracy: 0.7137 - loss: 0.6931 - val_accuracy: 0.8362 - val_loss: 0.4520\n",
      "Epoch 4/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 466us/step - accuracy: 0.7200 - loss: 0.6781 - val_accuracy: 0.8289 - val_loss: 0.4996\n",
      "Epoch 5/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 628us/step - accuracy: 0.7246 - loss: 0.6651 - val_accuracy: 0.6620 - val_loss: 1.9659\n",
      "Epoch 6/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 621us/step - accuracy: 0.7272 - loss: 0.6580 - val_accuracy: 0.6289 - val_loss: 3.8935\n",
      "Epoch 7/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 509us/step - accuracy: 0.7292 - loss: 0.6518 - val_accuracy: 0.6236 - val_loss: 5.5518\n",
      "Epoch 8/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 484us/step - accuracy: 0.7312 - loss: 0.6476 - val_accuracy: 0.6770 - val_loss: 2.6197\n",
      "Epoch 9/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 583us/step - accuracy: 0.7328 - loss: 0.6419 - val_accuracy: 0.6977 - val_loss: 3.3446\n",
      "Epoch 10/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 574us/step - accuracy: 0.7360 - loss: 0.6352 - val_accuracy: 0.6960 - val_loss: 2.3352\n",
      "Epoch 11/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 506us/step - accuracy: 0.7371 - loss: 0.6319 - val_accuracy: 0.6198 - val_loss: 6.5046\n",
      "Epoch 12/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 504us/step - accuracy: 0.7377 - loss: 0.6295 - val_accuracy: 0.6700 - val_loss: 3.5822\n",
      "Epoch 13/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 510us/step - accuracy: 0.7369 - loss: 0.6301 - val_accuracy: 0.8356 - val_loss: 0.8018\n",
      "Epoch 14/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 514us/step - accuracy: 0.7394 - loss: 0.6217 - val_accuracy: 0.8354 - val_loss: 0.8362\n",
      "Epoch 15/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 520us/step - accuracy: 0.7357 - loss: 0.6295 - val_accuracy: 0.8324 - val_loss: 0.7047\n",
      "Epoch 16/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 521us/step - accuracy: 0.7380 - loss: 0.6239 - val_accuracy: 0.7232 - val_loss: 1.6107\n",
      "Epoch 17/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 502us/step - accuracy: 0.7308 - loss: 0.6453 - val_accuracy: 0.6763 - val_loss: 5.5803\n",
      "Epoch 18/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 508us/step - accuracy: 0.7363 - loss: 0.6254 - val_accuracy: 0.8285 - val_loss: 1.2924\n",
      "Epoch 19/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 573us/step - accuracy: 0.7360 - loss: 0.6322 - val_accuracy: 0.6281 - val_loss: 2.4324\n",
      "Epoch 20/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 522us/step - accuracy: 0.7347 - loss: 0.6325 - val_accuracy: 0.8286 - val_loss: 1.5296\n",
      "Epoch 21/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 610us/step - accuracy: 0.7420 - loss: 0.6201 - val_accuracy: 0.6991 - val_loss: 7.3042\n",
      "Epoch 22/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 580us/step - accuracy: 0.7454 - loss: 0.6087 - val_accuracy: 0.8369 - val_loss: 1.0146\n",
      "Epoch 23/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 589us/step - accuracy: 0.7364 - loss: 0.6254 - val_accuracy: 0.8322 - val_loss: 0.8266\n",
      "Epoch 24/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 632us/step - accuracy: 0.7431 - loss: 0.6117 - val_accuracy: 0.8346 - val_loss: 0.9746\n",
      "Epoch 25/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 534us/step - accuracy: 0.7379 - loss: 0.6209 - val_accuracy: 0.8349 - val_loss: 1.1206\n",
      "Epoch 26/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 538us/step - accuracy: 0.7447 - loss: 0.6049 - val_accuracy: 0.8377 - val_loss: 1.1184\n",
      "Epoch 27/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 506us/step - accuracy: 0.7419 - loss: 0.6096 - val_accuracy: 0.8337 - val_loss: 1.5424\n",
      "Epoch 28/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 534us/step - accuracy: 0.7444 - loss: 0.6081 - val_accuracy: 0.8427 - val_loss: 1.0137\n",
      "Epoch 29/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 539us/step - accuracy: 0.7418 - loss: 0.6116 - val_accuracy: 0.8077 - val_loss: 1.6947\n",
      "Epoch 30/30\n",
      "\u001b[1m17500/17500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 511us/step - accuracy: 0.7451 - loss: 0.6034 - val_accuracy: 0.8394 - val_loss: 0.9647\n",
      "\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270us/step\n",
      "MLP Model Accuracy on Clean Data: 0.8394427440120488\n",
      "\n",
      "Classification Report on Clean Data:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.00      0.00      0.00       677\n",
      "      Backdoor       0.06      0.96      0.12       583\n",
      "           DoS       0.33      0.07      0.11      4089\n",
      "      Exploits       0.85      0.52      0.64     11132\n",
      "       Fuzzers       0.86      0.68      0.76      6062\n",
      "       Generic       1.00      0.96      0.98     18871\n",
      "        Normal       1.00      1.00      1.00     37000\n",
      "Reconnaissance       0.84      0.85      0.84      3496\n",
      "     Shellcode       0.21      0.87      0.34       378\n",
      "         Worms       0.08      0.77      0.15        44\n",
      "\n",
      "      accuracy                           0.84     82332\n",
      "     macro avg       0.52      0.67      0.49     82332\n",
      "  weighted avg       0.91      0.84      0.86     82332\n",
      "\n",
      "\n",
      "--- FGSM Attack ---\n",
      "\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265us/step\n",
      "FGSM Adversarial Accuracy: 0.5988437059709469\n",
      "\n",
      "Classification Report on FGSM Adversarial Examples:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.03      0.03      0.03       677\n",
      "      Backdoor       0.05      0.89      0.10       583\n",
      "           DoS       0.04      0.11      0.06      4089\n",
      "      Exploits       0.70      0.26      0.38     11132\n",
      "       Fuzzers       0.55      0.30      0.39      6062\n",
      "       Generic       0.96      0.30      0.45     18871\n",
      "        Normal       0.97      1.00      0.99     37000\n",
      "Reconnaissance       0.12      0.25      0.16      3496\n",
      "     Shellcode       0.06      0.38      0.11       378\n",
      "         Worms       0.03      0.25      0.05        44\n",
      "\n",
      "      accuracy                           0.60     82332\n",
      "     macro avg       0.35      0.38      0.27     82332\n",
      "  weighted avg       0.80      0.60      0.64     82332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, QuantileTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, CarliniL2Method\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "\n",
    "# Assuming train_data and test_data are available\n",
    "X_train = train_data.drop(columns=['attack_cat'])\n",
    "y_train = train_data['attack_cat']\n",
    "X_test = test_data.drop(columns=['attack_cat'])\n",
    "y_test = test_data['attack_cat']\n",
    "\n",
    "# Step 1: One-hot encode categorical features\n",
    "categorical_features = ['proto', 'service', 'state']\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Step 2: Encode the target labels (attack category)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 3: Handle class imbalance with oversampling\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train_encoded)\n",
    "\n",
    "# Step 4: Scale features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "X_train_scaled = quantile_transformer.fit_transform(X_train_scaled)\n",
    "X_test_scaled = quantile_transformer.transform(X_test_scaled)\n",
    "\n",
    "# Step 5: Define and compile the MLP model\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the model\n",
    "history = mlp_model.fit(X_train_scaled, y_train_resampled, epochs=30, batch_size=32, validation_data=(X_test_scaled, y_test_encoded))\n",
    "\n",
    "# Step 7: Evaluate the model on clean data\n",
    "y_pred = mlp_model.predict(X_test_scaled).argmax(axis=1)\n",
    "clean_accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "clean_class_report = classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_, zero_division=1)\n",
    "print(\"MLP Model Accuracy on Clean Data:\", clean_accuracy)\n",
    "print(\"\\nClassification Report on Clean Data:\\n\", clean_class_report)\n",
    "\n",
    "# Define critical features based on SHAP \n",
    "critical_features = [40, 174, 159, 31, 25] \n",
    "mask = np.zeros(X_train_scaled.shape[1], dtype=np.float32)\n",
    "mask[critical_features] = 1.0\n",
    "\n",
    "# Define the ART classifier\n",
    "classifier = TensorFlowV2Classifier(\n",
    "    model=mlp_model,\n",
    "    nb_classes=num_classes,\n",
    "    input_shape=(X_train_scaled.shape[1],),\n",
    "    loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    ")\n",
    "\n",
    "# Step 8: FGSM Attack\n",
    "print(\"\\n--- FGSM Attack ---\")\n",
    "fgsm = FastGradientMethod(estimator=classifier, eps=3.0, targeted=False)\n",
    "X_test_adv_fgsm = fgsm.generate(X_test_scaled, mask=mask)\n",
    "fgsm_preds = mlp_model.predict(X_test_adv_fgsm).argmax(axis=1)\n",
    "fgsm_accuracy = accuracy_score(y_test_encoded, fgsm_preds)\n",
    "fgsm_class_report = classification_report(y_test_encoded, fgsm_preds, target_names=label_encoder.classes_, zero_division=1)\n",
    "print(f\"FGSM Adversarial Accuracy: {fgsm_accuracy}\")\n",
    "print(f\"\\nClassification Report on FGSM Adversarial Examples:\\n{fgsm_class_report}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "26e7f9a5-d700-4d50-af60-74231df8580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PGD Attack ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step\n",
      "PGD Adversarial Accuracy: 0.6453262401010542\n",
      "\n",
      "Classification Report on PGD Adversarial Examples:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.03      0.13      0.05       677\n",
      "      Backdoor       0.06      0.90      0.12       583\n",
      "           DoS       0.13      0.10      0.11      4089\n",
      "      Exploits       0.69      0.25      0.37     11132\n",
      "       Fuzzers       0.55      0.25      0.34      6062\n",
      "       Generic       0.99      0.52      0.68     18871\n",
      "        Normal       0.98      1.00      0.99     37000\n",
      "Reconnaissance       0.09      0.24      0.13      3496\n",
      "     Shellcode       0.05      0.43      0.10       378\n",
      "         Worms       0.02      0.32      0.03        44\n",
      "\n",
      "      accuracy                           0.65     82332\n",
      "     macro avg       0.36      0.41      0.29     82332\n",
      "  weighted avg       0.81      0.65      0.69     82332\n",
      "\n",
      "\n",
      "--- Carlini & Wagner Attack (on a subset) ---\n"
     ]
    }
   ],
   "source": [
    "# Step 9: PGD Attack\n",
    "print(\"\\n--- PGD Attack ---\")\n",
    "pgd = ProjectedGradientDescent(estimator=classifier, eps=3.0, eps_step=0.1, max_iter=100, targeted=False)\n",
    "X_test_adv_pgd = pgd.generate(X_test_scaled, mask=mask)\n",
    "pgd_preds = mlp_model.predict(X_test_adv_pgd).argmax(axis=1)\n",
    "pgd_accuracy = accuracy_score(y_test_encoded, pgd_preds)\n",
    "pgd_class_report = classification_report(y_test_encoded, pgd_preds, target_names=label_encoder.classes_, zero_division=1)\n",
    "print(f\"PGD Adversarial Accuracy: {pgd_accuracy}\")\n",
    "print(f\"\\nClassification Report on PGD Adversarial Examples:\\n{pgd_class_report}\")\n",
    "print(\"\\n--- Carlini & Wagner Attack (on a subset) ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3fa74143-85ea-4e4a-9d88-58f1ab389a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bde920ffa742fa81e2ef51c0e1932c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step\n",
      "Carlini & Wagner Adversarial Accuracy (Subset): 0.631578947368421\n",
      "\n",
      "Classification Report on Carlini & Wagner Adversarial Examples (Subset):\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.00      0.00      0.00        50\n",
      "      Backdoor       0.28      0.86      0.42        50\n",
      "           DoS       0.57      0.08      0.14        50\n",
      "      Exploits       0.60      0.36      0.45        50\n",
      "       Fuzzers       0.76      0.96      0.85        50\n",
      "       Generic       1.00      0.96      0.98        50\n",
      "        Normal       0.83      1.00      0.91        50\n",
      "Reconnaissance       0.84      0.64      0.73        50\n",
      "     Shellcode       0.77      0.72      0.74        50\n",
      "         Worms       0.97      0.75      0.85        44\n",
      "\n",
      "      accuracy                           0.63       494\n",
      "     macro avg       0.66      0.63      0.61       494\n",
      "  weighted avg       0.66      0.63      0.60       494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select a subset that includes multiple classes\n",
    "subset_indices = []\n",
    "classes_to_include = np.unique(y_test_encoded)\n",
    "for cls in classes_to_include:\n",
    "    indices = np.where(y_test_encoded == cls)[0][:50]  # Take up to 50 samples per class\n",
    "    subset_indices.extend(indices)\n",
    "\n",
    "# Create a subset based on the selected indices\n",
    "subset_indices = np.array(subset_indices)\n",
    "X_test_subset = X_test_scaled[subset_indices]\n",
    "y_test_subset = y_test_encoded[subset_indices]\n",
    "\n",
    "# Run Carlini & Wagner attack on the subset\n",
    "cw = CarliniL2Method(classifier=classifier, confidence=1.0, targeted=False, max_iter=100)\n",
    "X_test_adv_cw_subset = cw.generate(X_test_subset, mask=mask)\n",
    "\n",
    "# Evaluate the attack\n",
    "cw_preds_subset = mlp_model.predict(X_test_adv_cw_subset).argmax(axis=1)\n",
    "cw_accuracy_subset = accuracy_score(y_test_subset, cw_preds_subset)\n",
    "cw_class_report_subset = classification_report(\n",
    "    y_test_subset, \n",
    "    cw_preds_subset, \n",
    "    target_names=label_encoder.classes_, \n",
    "    zero_division=1\n",
    ")\n",
    "\n",
    "print(f\"Carlini & Wagner Adversarial Accuracy (Subset): {cw_accuracy_subset}\")\n",
    "print(f\"\\nClassification Report on Carlini & Wagner Adversarial Examples (Subset):\\n{cw_class_report_subset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7bfb5d-be30-461f-aa4b-30016e3cb840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
