{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915c09e8-8287-4a37-9bdc-18a5467ea852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad776770-e282-43ae-927c-c97b05a95036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/Users/marlenawasiak/Desktop/Data_Collection/NSL_KDD_Train.csv')\n",
    "test_data = pd.read_csv('/Users/marlenawasiak/Desktop/Data_Collection/NSL_KDD_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1607f7-9277-4250-968b-472bfea7165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.iloc[:, :-1]\n",
    "y_train = train_data.iloc[:, -1]\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "# Align columns to keep only the common columns\n",
    "common_columns = X_train.columns.intersection(X_test.columns)\n",
    "X_train = X_train[common_columns]\n",
    "X_test = X_test[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933dfcbe-7481-41d8-83f8-8a6109cf6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels with combined categories\n",
    "all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1ba617-6ac1-48b7-b528-a1a1e8f2c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature ranges after RobustScaler:\n",
      "Train set min: -1.0989010989010988 max: 42908.0\n",
      "Test set min: -1.0989010989010988 max: 57715.0\n",
      "Scaled feature ranges after QuantileTransformer and clipping:\n",
      "Train set min: -3.0 max: 3.0\n",
      "Test set min: -3.0 max: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical features with consistent categories\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    combined_categories = pd.concat([X_train[col], X_test[col]], axis=0).astype(\"category\").cat.categories\n",
    "    X_train[col] = pd.Categorical(X_train[col], categories=combined_categories)\n",
    "    X_test[col] = pd.Categorical(X_test[col], categories=combined_categories)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    X_test[col] = le.transform(X_test[col].astype(str))\n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Define clipping thresholds based on the 99th percentile of the training data\n",
    "clip_thresholds = {\n",
    "    \"0.1\": X_train[\"0.1\"].quantile(0.99),\n",
    "    \"0.2\": X_train[\"0.2\"].quantile(0.99)\n",
    "}\n",
    "\n",
    "# Clip the training and test data at these thresholds\n",
    "X_train_clipped = X_train.copy()\n",
    "X_test_clipped = X_test.copy()\n",
    "\n",
    "for col, threshold in clip_thresholds.items():\n",
    "    X_train_clipped[col] = X_train[col].clip(upper=threshold)\n",
    "    X_test_clipped[col] = X_test[col].clip(upper=threshold)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Fit RobustScaler on the training data\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clipped)\n",
    "X_test_scaled = scaler.transform(X_test_clipped)\n",
    "\n",
    "# Verify the new scaled ranges\n",
    "print(\"Scaled feature ranges after RobustScaler:\")\n",
    "print(\"Train set min:\", X_train_scaled.min(), \"max:\", X_train_scaled.max())\n",
    "print(\"Test set min:\", X_test_scaled.min(), \"max:\", X_test_scaled.max())\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Apply QuantileTransformer with a normal distribution\n",
    "scaler = QuantileTransformer(output_distribution='normal')  # Alternatively, try 'uniform' if normal doesn't work well\n",
    "X_train_scaled = scaler.fit_transform(X_train_clipped)\n",
    "X_test_scaled = scaler.transform(X_test_clipped)\n",
    "\n",
    "# Optional: Clip values to remove any remaining extreme values\n",
    "X_train_scaled = np.clip(X_train_scaled, -3, 3)  # Adjust the range as needed\n",
    "X_test_scaled = np.clip(X_test_scaled, -3, 3)\n",
    "\n",
    "# Verify the scaled ranges\n",
    "print(\"Scaled feature ranges after QuantileTransformer and clipping:\")\n",
    "print(\"Train set min:\", X_train_scaled.min(), \"max:\", X_train_scaled.max())\n",
    "print(\"Test set min:\", X_test_scaled.min(), \"max:\", X_test_scaled.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3f882741-136f-44e7-9b9c-56a936e1f996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 517us/step - accuracy: 0.8514 - loss: 42.3309 - val_accuracy: 0.4778 - val_loss: 7.1463\n",
      "Epoch 2/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 486us/step - accuracy: 0.9143 - loss: 9.9232 - val_accuracy: 0.4743 - val_loss: 7.4399\n",
      "Epoch 3/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 486us/step - accuracy: 0.9210 - loss: 9.0000 - val_accuracy: 0.4027 - val_loss: 9.9748\n",
      "Epoch 4/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490us/step - accuracy: 0.9280 - loss: 8.2712 - val_accuracy: 0.3435 - val_loss: 10.1673\n",
      "Epoch 5/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505us/step - accuracy: 0.9283 - loss: 8.5306 - val_accuracy: 0.3939 - val_loss: 9.1332\n",
      "Epoch 6/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 485us/step - accuracy: 0.9311 - loss: 7.9169 - val_accuracy: 0.4222 - val_loss: 9.8036\n",
      "Epoch 7/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490us/step - accuracy: 0.9304 - loss: 7.5379 - val_accuracy: 0.3403 - val_loss: 11.6714\n",
      "Epoch 8/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483us/step - accuracy: 0.9315 - loss: 8.0190 - val_accuracy: 0.3446 - val_loss: 11.8494\n",
      "Epoch 9/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 485us/step - accuracy: 0.9321 - loss: 7.5979 - val_accuracy: 0.3649 - val_loss: 13.0265\n",
      "Epoch 10/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 486us/step - accuracy: 0.9317 - loss: 7.3850 - val_accuracy: 0.3483 - val_loss: 11.8900\n",
      "Epoch 11/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 486us/step - accuracy: 0.9321 - loss: 7.7505 - val_accuracy: 0.3548 - val_loss: 17.0924\n",
      "Epoch 12/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 491us/step - accuracy: 0.9315 - loss: 6.9742 - val_accuracy: 0.4046 - val_loss: 13.5693\n",
      "Epoch 13/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495us/step - accuracy: 0.9329 - loss: 6.6553 - val_accuracy: 0.3611 - val_loss: 14.0738\n",
      "Epoch 14/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507us/step - accuracy: 0.9329 - loss: 6.6071 - val_accuracy: 0.3621 - val_loss: 15.2912\n",
      "Epoch 15/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 516us/step - accuracy: 0.9308 - loss: 7.1227 - val_accuracy: 0.4806 - val_loss: 16.9254\n",
      "Epoch 16/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 519us/step - accuracy: 0.9337 - loss: 6.8014 - val_accuracy: 0.3543 - val_loss: 17.2300\n",
      "Epoch 17/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 501us/step - accuracy: 0.9362 - loss: 6.5925 - val_accuracy: 0.3629 - val_loss: 20.7273\n",
      "Epoch 18/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490us/step - accuracy: 0.9336 - loss: 6.6040 - val_accuracy: 0.2491 - val_loss: 17.1227\n",
      "Epoch 19/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 493us/step - accuracy: 0.9343 - loss: 6.6972 - val_accuracy: 0.2926 - val_loss: 16.7415\n",
      "Epoch 20/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step - accuracy: 0.9361 - loss: 6.5292 - val_accuracy: 0.3469 - val_loss: 18.4416\n",
      "Epoch 21/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537us/step - accuracy: 0.9323 - loss: 6.3192 - val_accuracy: 0.3417 - val_loss: 21.9866\n",
      "Epoch 22/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step - accuracy: 0.9342 - loss: 5.9925 - val_accuracy: 0.3504 - val_loss: 20.8266\n",
      "Epoch 23/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 515us/step - accuracy: 0.9329 - loss: 7.2732 - val_accuracy: 0.3251 - val_loss: 20.0425\n",
      "Epoch 24/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 499us/step - accuracy: 0.9329 - loss: 6.6089 - val_accuracy: 0.2815 - val_loss: 22.7898\n",
      "Epoch 25/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504us/step - accuracy: 0.9355 - loss: 5.7703 - val_accuracy: 0.1672 - val_loss: 22.8658\n",
      "Epoch 26/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495us/step - accuracy: 0.9350 - loss: 5.7921 - val_accuracy: 0.3286 - val_loss: 20.8494\n",
      "Epoch 27/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 501us/step - accuracy: 0.9363 - loss: 6.3024 - val_accuracy: 0.3304 - val_loss: 21.5283\n",
      "Epoch 28/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 514us/step - accuracy: 0.9348 - loss: 6.3834 - val_accuracy: 0.3189 - val_loss: 21.4830\n",
      "Epoch 29/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470us/step - accuracy: 0.9361 - loss: 5.6463 - val_accuracy: 0.3315 - val_loss: 23.9820\n",
      "Epoch 30/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 464us/step - accuracy: 0.9345 - loss: 5.5404 - val_accuracy: 0.3339 - val_loss: 29.1598\n",
      "Epoch 31/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468us/step - accuracy: 0.9349 - loss: 6.2466 - val_accuracy: 0.3355 - val_loss: 23.6444\n",
      "Epoch 32/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 475us/step - accuracy: 0.9343 - loss: 5.7145 - val_accuracy: 0.4777 - val_loss: 29.5090\n",
      "Epoch 33/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458us/step - accuracy: 0.9361 - loss: 5.7550 - val_accuracy: 0.3305 - val_loss: 20.8097\n",
      "Epoch 34/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 465us/step - accuracy: 0.9365 - loss: 5.9847 - val_accuracy: 0.3328 - val_loss: 32.7351\n",
      "Epoch 35/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456us/step - accuracy: 0.9370 - loss: 6.8200 - val_accuracy: 0.3363 - val_loss: 32.9045\n",
      "Epoch 36/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 463us/step - accuracy: 0.9345 - loss: 5.9592 - val_accuracy: 0.3278 - val_loss: 31.4624\n",
      "Epoch 37/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 457us/step - accuracy: 0.9375 - loss: 5.7356 - val_accuracy: 0.3211 - val_loss: 31.1015\n",
      "Epoch 38/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step - accuracy: 0.9349 - loss: 5.7888 - val_accuracy: 0.3863 - val_loss: 26.6229\n",
      "Epoch 39/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 465us/step - accuracy: 0.9341 - loss: 6.2618 - val_accuracy: 0.3303 - val_loss: 25.7226\n",
      "Epoch 40/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 469us/step - accuracy: 0.9359 - loss: 5.5589 - val_accuracy: 0.3237 - val_loss: 29.0420\n",
      "Epoch 41/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 469us/step - accuracy: 0.9359 - loss: 5.7847 - val_accuracy: 0.3111 - val_loss: 40.9479\n",
      "Epoch 42/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466us/step - accuracy: 0.9334 - loss: 6.0166 - val_accuracy: 0.3282 - val_loss: 30.6700\n",
      "Epoch 43/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473us/step - accuracy: 0.9352 - loss: 6.2230 - val_accuracy: 0.3511 - val_loss: 29.6437\n",
      "Epoch 44/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step - accuracy: 0.9342 - loss: 6.1800 - val_accuracy: 0.3237 - val_loss: 31.2668\n",
      "Epoch 45/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467us/step - accuracy: 0.9357 - loss: 5.9091 - val_accuracy: 0.3313 - val_loss: 38.5384\n",
      "Epoch 46/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470us/step - accuracy: 0.9341 - loss: 6.5141 - val_accuracy: 0.3889 - val_loss: 33.3242\n",
      "Epoch 47/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467us/step - accuracy: 0.9355 - loss: 5.4462 - val_accuracy: 0.3262 - val_loss: 36.1593\n",
      "Epoch 48/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466us/step - accuracy: 0.9350 - loss: 5.1621 - val_accuracy: 0.3202 - val_loss: 36.6028\n",
      "Epoch 49/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466us/step - accuracy: 0.9359 - loss: 5.9771 - val_accuracy: 0.3740 - val_loss: 37.7161\n",
      "Epoch 50/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466us/step - accuracy: 0.9351 - loss: 6.0709 - val_accuracy: 0.3964 - val_loss: 33.4284\n",
      "Epoch 51/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471us/step - accuracy: 0.9311 - loss: 5.4655 - val_accuracy: 0.3828 - val_loss: 39.6815\n",
      "Epoch 52/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 465us/step - accuracy: 0.9344 - loss: 5.7626 - val_accuracy: 0.3261 - val_loss: 33.9275\n",
      "Epoch 53/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 474us/step - accuracy: 0.9360 - loss: 5.1853 - val_accuracy: 0.3567 - val_loss: 34.3890\n",
      "Epoch 54/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468us/step - accuracy: 0.9358 - loss: 5.4459 - val_accuracy: 0.4020 - val_loss: 37.1825\n",
      "Epoch 55/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 531us/step - accuracy: 0.9339 - loss: 6.2816 - val_accuracy: 0.3980 - val_loss: 35.2726\n",
      "Epoch 56/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 499us/step - accuracy: 0.9358 - loss: 5.8695 - val_accuracy: 0.3993 - val_loss: 34.2127\n",
      "Epoch 57/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495us/step - accuracy: 0.9377 - loss: 5.7014 - val_accuracy: 0.3848 - val_loss: 35.7957\n",
      "Epoch 58/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 510us/step - accuracy: 0.9355 - loss: 5.8035 - val_accuracy: 0.3571 - val_loss: 29.3430\n",
      "Epoch 59/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 529us/step - accuracy: 0.9341 - loss: 5.6028 - val_accuracy: 0.3395 - val_loss: 37.6389\n",
      "Epoch 60/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 506us/step - accuracy: 0.9363 - loss: 5.0587 - val_accuracy: 0.4026 - val_loss: 42.9985\n",
      "Epoch 61/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 479us/step - accuracy: 0.9356 - loss: 5.5348 - val_accuracy: 0.2858 - val_loss: 43.4008\n",
      "Epoch 62/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468us/step - accuracy: 0.9318 - loss: 6.8214 - val_accuracy: 0.3273 - val_loss: 49.6479\n",
      "Epoch 63/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step - accuracy: 0.9361 - loss: 5.2906 - val_accuracy: 0.3246 - val_loss: 55.1413\n",
      "Epoch 64/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step - accuracy: 0.9371 - loss: 5.3843 - val_accuracy: 0.3922 - val_loss: 42.0970\n",
      "Epoch 65/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 500us/step - accuracy: 0.9356 - loss: 5.3564 - val_accuracy: 0.3830 - val_loss: 34.2214\n",
      "Epoch 66/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step - accuracy: 0.9347 - loss: 5.3577 - val_accuracy: 0.4015 - val_loss: 45.6896\n",
      "Epoch 67/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706us/step - accuracy: 0.9348 - loss: 5.2895 - val_accuracy: 0.3255 - val_loss: 40.9000\n",
      "Epoch 68/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 510us/step - accuracy: 0.9334 - loss: 5.2645 - val_accuracy: 0.3894 - val_loss: 42.5044\n",
      "Epoch 69/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667us/step - accuracy: 0.9383 - loss: 5.5130 - val_accuracy: 0.3891 - val_loss: 48.6537\n",
      "Epoch 70/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709us/step - accuracy: 0.9350 - loss: 5.9464 - val_accuracy: 0.2827 - val_loss: 37.0110\n",
      "Epoch 71/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 524us/step - accuracy: 0.9372 - loss: 5.5880 - val_accuracy: 0.2605 - val_loss: 41.5654\n",
      "Epoch 72/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 519us/step - accuracy: 0.9346 - loss: 6.0168 - val_accuracy: 0.3110 - val_loss: 36.1781\n",
      "Epoch 73/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 532us/step - accuracy: 0.9366 - loss: 6.0723 - val_accuracy: 0.3487 - val_loss: 33.5343\n",
      "Epoch 74/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 481us/step - accuracy: 0.9339 - loss: 6.5507 - val_accuracy: 0.3287 - val_loss: 39.8290\n",
      "Epoch 75/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 550us/step - accuracy: 0.9371 - loss: 5.5157 - val_accuracy: 0.3826 - val_loss: 42.5480\n",
      "Epoch 76/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 526us/step - accuracy: 0.9369 - loss: 5.2480 - val_accuracy: 0.3935 - val_loss: 43.5830\n",
      "Epoch 77/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497us/step - accuracy: 0.9363 - loss: 6.8365 - val_accuracy: 0.3321 - val_loss: 50.9591\n",
      "Epoch 78/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483us/step - accuracy: 0.9337 - loss: 5.6863 - val_accuracy: 0.3348 - val_loss: 61.7665\n",
      "Epoch 79/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 477us/step - accuracy: 0.9353 - loss: 5.2475 - val_accuracy: 0.2835 - val_loss: 40.7106\n",
      "Epoch 80/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 472us/step - accuracy: 0.9347 - loss: 6.5502 - val_accuracy: 0.3932 - val_loss: 43.4004\n",
      "Epoch 81/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 477us/step - accuracy: 0.9357 - loss: 5.6059 - val_accuracy: 0.3412 - val_loss: 49.3140\n",
      "Epoch 82/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470us/step - accuracy: 0.9351 - loss: 5.2285 - val_accuracy: 0.4383 - val_loss: 44.1325\n",
      "Epoch 83/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 474us/step - accuracy: 0.9323 - loss: 5.5883 - val_accuracy: 0.3896 - val_loss: 47.8227\n",
      "Epoch 84/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 480us/step - accuracy: 0.9345 - loss: 5.6764 - val_accuracy: 0.3992 - val_loss: 45.9528\n",
      "Epoch 85/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 479us/step - accuracy: 0.9350 - loss: 5.4717 - val_accuracy: 0.3668 - val_loss: 46.8151\n",
      "Epoch 86/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 503us/step - accuracy: 0.9316 - loss: 5.7428 - val_accuracy: 0.3831 - val_loss: 51.5370\n",
      "Epoch 87/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473us/step - accuracy: 0.9359 - loss: 5.6695 - val_accuracy: 0.3704 - val_loss: 51.2467\n",
      "Epoch 88/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471us/step - accuracy: 0.9361 - loss: 5.7545 - val_accuracy: 0.1636 - val_loss: 48.7339\n",
      "Epoch 89/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471us/step - accuracy: 0.9319 - loss: 6.0396 - val_accuracy: 0.2558 - val_loss: 82.7610\n",
      "Epoch 90/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 480us/step - accuracy: 0.9346 - loss: 6.4654 - val_accuracy: 0.3873 - val_loss: 59.8150\n",
      "Epoch 91/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 485us/step - accuracy: 0.9368 - loss: 5.1935 - val_accuracy: 0.3873 - val_loss: 65.9946\n",
      "Epoch 92/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483us/step - accuracy: 0.9364 - loss: 5.8167 - val_accuracy: 0.3167 - val_loss: 59.3199\n",
      "Epoch 93/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 494us/step - accuracy: 0.9340 - loss: 5.0359 - val_accuracy: 0.3969 - val_loss: 53.4907\n",
      "Epoch 94/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482us/step - accuracy: 0.9357 - loss: 5.6119 - val_accuracy: 0.3373 - val_loss: 65.0613\n",
      "Epoch 95/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476us/step - accuracy: 0.9340 - loss: 10.4609 - val_accuracy: 0.2848 - val_loss: 64.5625\n",
      "Epoch 96/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 530us/step - accuracy: 0.9363 - loss: 5.5782 - val_accuracy: 0.3143 - val_loss: 75.4579\n",
      "Epoch 97/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504us/step - accuracy: 0.9351 - loss: 5.9523 - val_accuracy: 0.3221 - val_loss: 62.1206\n",
      "Epoch 98/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 485us/step - accuracy: 0.9339 - loss: 5.6258 - val_accuracy: 0.3853 - val_loss: 68.1328\n",
      "Epoch 99/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 481us/step - accuracy: 0.9332 - loss: 5.8267 - val_accuracy: 0.3058 - val_loss: 67.4130\n",
      "Epoch 100/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513us/step - accuracy: 0.9363 - loss: 5.2905 - val_accuracy: 0.2177 - val_loss: 68.2776\n",
      "Epoch 101/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 492us/step - accuracy: 0.9371 - loss: 5.8972 - val_accuracy: 0.2863 - val_loss: 58.8101\n",
      "Epoch 102/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - accuracy: 0.9367 - loss: 5.7912 - val_accuracy: 0.2075 - val_loss: 67.7200\n",
      "Epoch 103/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 501us/step - accuracy: 0.9365 - loss: 5.7402 - val_accuracy: 0.2085 - val_loss: 65.4215\n",
      "Epoch 104/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505us/step - accuracy: 0.9372 - loss: 6.3858 - val_accuracy: 0.2300 - val_loss: 78.2942\n",
      "Epoch 105/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 487us/step - accuracy: 0.9340 - loss: 5.1712 - val_accuracy: 0.2324 - val_loss: 60.3794\n",
      "Epoch 106/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482us/step - accuracy: 0.9370 - loss: 5.7594 - val_accuracy: 0.3019 - val_loss: 68.7266\n",
      "Epoch 107/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 514us/step - accuracy: 0.9363 - loss: 5.4290 - val_accuracy: 0.3368 - val_loss: 68.2203\n",
      "Epoch 108/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 486us/step - accuracy: 0.9300 - loss: 5.2869 - val_accuracy: 0.3314 - val_loss: 69.8360\n",
      "Epoch 109/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 511us/step - accuracy: 0.9330 - loss: 5.5986 - val_accuracy: 0.2298 - val_loss: 61.5096\n",
      "Epoch 110/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484us/step - accuracy: 0.9362 - loss: 5.3820 - val_accuracy: 0.1582 - val_loss: 56.0962\n",
      "Epoch 111/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 481us/step - accuracy: 0.9369 - loss: 4.9950 - val_accuracy: 0.1594 - val_loss: 54.1927\n",
      "Epoch 112/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 485us/step - accuracy: 0.9336 - loss: 6.2684 - val_accuracy: 0.3651 - val_loss: 58.7528\n",
      "Epoch 113/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497us/step - accuracy: 0.9383 - loss: 4.9605 - val_accuracy: 0.3336 - val_loss: 62.1551\n",
      "Epoch 114/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513us/step - accuracy: 0.9382 - loss: 5.4120 - val_accuracy: 0.1746 - val_loss: 57.7803\n",
      "Epoch 115/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 526us/step - accuracy: 0.9348 - loss: 5.2943 - val_accuracy: 0.3897 - val_loss: 63.8940\n",
      "Epoch 116/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484us/step - accuracy: 0.9362 - loss: 5.1825 - val_accuracy: 0.3458 - val_loss: 74.2046\n",
      "Epoch 117/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step - accuracy: 0.9359 - loss: 4.8843 - val_accuracy: 0.3420 - val_loss: 92.5649\n",
      "Epoch 118/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 491us/step - accuracy: 0.9363 - loss: 6.4598 - val_accuracy: 0.3195 - val_loss: 98.2207\n",
      "Epoch 119/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490us/step - accuracy: 0.9372 - loss: 6.0746 - val_accuracy: 0.3440 - val_loss: 97.9586\n",
      "Epoch 120/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513us/step - accuracy: 0.9338 - loss: 6.4763 - val_accuracy: 0.4552 - val_loss: 82.0332\n",
      "Epoch 121/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476us/step - accuracy: 0.9376 - loss: 5.2346 - val_accuracy: 0.3341 - val_loss: 80.6404\n",
      "Epoch 122/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step - accuracy: 0.9373 - loss: 5.1003 - val_accuracy: 0.3861 - val_loss: 92.0373\n",
      "Epoch 123/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 477us/step - accuracy: 0.9364 - loss: 5.9667 - val_accuracy: 0.2611 - val_loss: 94.1683\n",
      "Epoch 124/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step - accuracy: 0.9368 - loss: 5.6952 - val_accuracy: 0.3511 - val_loss: 91.0785\n",
      "Epoch 125/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484us/step - accuracy: 0.9371 - loss: 5.2077 - val_accuracy: 0.1881 - val_loss: 82.0601\n",
      "Epoch 126/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 475us/step - accuracy: 0.9374 - loss: 5.5225 - val_accuracy: 0.4007 - val_loss: 95.5565\n",
      "Epoch 127/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 487us/step - accuracy: 0.9368 - loss: 6.3412 - val_accuracy: 0.3019 - val_loss: 99.1411\n",
      "Epoch 128/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473us/step - accuracy: 0.9365 - loss: 6.2384 - val_accuracy: 0.3418 - val_loss: 101.4134\n",
      "Epoch 129/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471us/step - accuracy: 0.9350 - loss: 5.0136 - val_accuracy: 0.2393 - val_loss: 105.1742\n",
      "Epoch 130/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467us/step - accuracy: 0.9366 - loss: 5.7611 - val_accuracy: 0.3028 - val_loss: 84.0535\n",
      "Epoch 131/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471us/step - accuracy: 0.9386 - loss: 5.0955 - val_accuracy: 0.3124 - val_loss: 91.1457\n",
      "Epoch 132/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 478us/step - accuracy: 0.9369 - loss: 5.2052 - val_accuracy: 0.3189 - val_loss: 96.6884\n",
      "Epoch 133/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476us/step - accuracy: 0.9379 - loss: 5.1238 - val_accuracy: 0.3868 - val_loss: 86.2785\n",
      "Epoch 134/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 480us/step - accuracy: 0.9353 - loss: 5.4232 - val_accuracy: 0.3381 - val_loss: 111.9297\n",
      "Epoch 135/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 480us/step - accuracy: 0.9365 - loss: 5.2879 - val_accuracy: 0.3880 - val_loss: 95.3487\n",
      "Epoch 136/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 499us/step - accuracy: 0.9366 - loss: 5.4910 - val_accuracy: 0.2960 - val_loss: 89.0186\n",
      "Epoch 137/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467us/step - accuracy: 0.9369 - loss: 5.0594 - val_accuracy: 0.3798 - val_loss: 113.9209\n",
      "Epoch 138/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473us/step - accuracy: 0.9369 - loss: 5.5507 - val_accuracy: 0.2559 - val_loss: 105.8618\n",
      "Epoch 139/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 493us/step - accuracy: 0.9379 - loss: 5.9487 - val_accuracy: 0.4115 - val_loss: 119.0657\n",
      "Epoch 140/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 491us/step - accuracy: 0.9354 - loss: 6.0444 - val_accuracy: 0.3953 - val_loss: 130.2829\n",
      "Epoch 141/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497us/step - accuracy: 0.9381 - loss: 6.2435 - val_accuracy: 0.3956 - val_loss: 144.7139\n",
      "Epoch 142/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step - accuracy: 0.9372 - loss: 5.8619 - val_accuracy: 0.3676 - val_loss: 142.5261\n",
      "Epoch 143/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 479us/step - accuracy: 0.9370 - loss: 5.4822 - val_accuracy: 0.3866 - val_loss: 128.7754\n",
      "Epoch 144/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 494us/step - accuracy: 0.9375 - loss: 5.4533 - val_accuracy: 0.3763 - val_loss: 140.0878\n",
      "Epoch 145/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 485us/step - accuracy: 0.9359 - loss: 5.6510 - val_accuracy: 0.3859 - val_loss: 136.1586\n",
      "Epoch 146/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 493us/step - accuracy: 0.9380 - loss: 5.3877 - val_accuracy: 0.3973 - val_loss: 108.4583\n",
      "Epoch 147/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497us/step - accuracy: 0.9379 - loss: 5.7722 - val_accuracy: 0.3968 - val_loss: 125.3138\n",
      "Epoch 148/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 503us/step - accuracy: 0.9357 - loss: 9.5832 - val_accuracy: 0.2673 - val_loss: 112.3145\n",
      "Epoch 149/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 499us/step - accuracy: 0.9373 - loss: 4.9640 - val_accuracy: 0.3894 - val_loss: 135.5299\n",
      "Epoch 150/150\n",
      "\u001b[1m1969/1969\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step - accuracy: 0.9380 - loss: 4.8763 - val_accuracy: 0.3283 - val_loss: 137.3138\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Define the neural network model\n",
    "# Experiment with a deeper model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train_encoded, epochs=150, batch_size=64, validation_data=(X_test_scaled, y_test_encoded), class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "363e766c-3bec-48df-a3ed-4eb7991fd3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step\n",
      "Test Accuracy: 0.32830590427183604\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        apache2       1.00      0.00      0.00       737\n",
      "           back       1.00      0.00      0.00       359\n",
      "buffer_overflow       0.00      0.00      0.00        20\n",
      "      ftp_write       1.00      0.00      0.00         3\n",
      "   guess_passwd       1.00      0.00      0.00      1231\n",
      "     httptunnel       1.00      0.00      0.00       133\n",
      "           imap       0.00      0.00      0.00         1\n",
      "        ipsweep       1.00      0.00      0.00       141\n",
      "           land       1.00      0.00      0.00         7\n",
      "     loadmodule       1.00      0.00      0.00         2\n",
      "       mailbomb       1.00      0.00      0.00       293\n",
      "          mscan       1.00      0.00      0.00       996\n",
      "       multihop       0.00      0.00      0.00        18\n",
      "          named       1.00      0.00      0.00        17\n",
      "        neptune       0.33      0.31      0.32      4656\n",
      "           nmap       0.00      0.00      0.00        73\n",
      "         normal       0.40      0.61      0.48      9711\n",
      "           perl       1.00      0.00      0.00         2\n",
      "            phf       1.00      0.00      0.00         2\n",
      "            pod       1.00      0.00      0.00        41\n",
      "      portsweep       0.00      0.02      0.00       157\n",
      "   processtable       1.00      0.00      0.00       685\n",
      "             ps       1.00      0.00      0.00        15\n",
      "        rootkit       1.00      0.00      0.00        13\n",
      "          saint       1.00      0.00      0.00       319\n",
      "          satan       0.00      0.00      0.00       735\n",
      "       sendmail       1.00      0.00      0.00        14\n",
      "          smurf       1.00      0.00      0.00       665\n",
      "  snmpgetattack       1.00      0.00      0.00       178\n",
      "      snmpguess       1.00      0.00      0.00       331\n",
      "            spy       1.00      1.00      1.00         0\n",
      "      sqlattack       1.00      0.00      0.00         2\n",
      "       teardrop       1.00      0.00      0.00        12\n",
      "       udpstorm       1.00      0.00      0.00         2\n",
      "    warezclient       0.00      1.00      0.00         0\n",
      "    warezmaster       1.00      0.00      0.00       944\n",
      "           worm       1.00      0.00      0.00         2\n",
      "          xlock       1.00      0.00      0.00         9\n",
      "         xsnoop       1.00      0.00      0.00         4\n",
      "          xterm       1.00      0.00      0.00        13\n",
      "\n",
      "      micro avg       0.33      0.33      0.33     22543\n",
      "      macro avg       0.79      0.07      0.05     22543\n",
      "   weighted avg       0.56      0.33      0.27     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred_encoded = model.predict(X_test_scaled).argmax(axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Generate classification report with specified labels to handle all classes\n",
    "class_report = classification_report(\n",
    "    y_test_encoded, \n",
    "    y_pred_encoded, \n",
    "    labels=range(len(label_encoder.classes_)), \n",
    "    target_names=label_encoder.classes_, \n",
    "    zero_division=1\n",
    ")\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9d513-b529-4e9e-8f75-3703c12ae87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c61b5a73-22c9-404d-aa54-4dd2dd66e839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FGSM Attack with Higher Epsilon ---\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step\n",
      "FGSM Adversarial Accuracy (eps=2.0): 0.2431797010158364\n",
      "FGSM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       737\n",
      "           1       1.00      0.00      0.00       359\n",
      "           2       1.00      0.00      0.00        20\n",
      "           3       1.00      0.00      0.00         3\n",
      "           4       1.00      0.00      0.00      1231\n",
      "           5       1.00      0.00      0.00       133\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00       141\n",
      "           8       1.00      0.00      0.00         7\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      0.00      0.00       293\n",
      "          11       1.00      0.00      0.00       996\n",
      "          12       0.00      0.00      0.00        18\n",
      "          13       1.00      0.00      0.00        17\n",
      "          14       0.28      0.33      0.30      4656\n",
      "          15       0.00      0.00      0.00        73\n",
      "          16       0.35      0.41      0.38      9711\n",
      "          17       1.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         2\n",
      "          19       1.00      0.00      0.00        41\n",
      "          20       0.00      0.01      0.00       157\n",
      "          21       1.00      0.00      0.00       685\n",
      "          22       1.00      0.00      0.00        15\n",
      "          23       1.00      0.00      0.00        13\n",
      "          24       1.00      0.00      0.00       319\n",
      "          25       0.00      0.00      0.00       735\n",
      "          26       1.00      0.00      0.00        14\n",
      "          27       1.00      0.00      0.00       665\n",
      "          28       1.00      0.00      0.00       178\n",
      "          29       1.00      0.00      0.00       331\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       1.00      0.00      0.00        12\n",
      "          33       1.00      0.00      0.00         2\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       1.00      0.00      0.00       944\n",
      "          36       1.00      0.00      0.00         2\n",
      "          37       1.00      0.00      0.00         9\n",
      "          38       1.00      0.00      0.00         4\n",
      "          39       1.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.24     22543\n",
      "   macro avg       0.79      0.04      0.02     22543\n",
      "weighted avg       0.52      0.24      0.22     22543\n",
      "\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286us/step\n",
      "FGSM Adversarial Accuracy (eps=3.0): 0.1508228718449186\n",
      "FGSM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       737\n",
      "           1       1.00      0.00      0.00       359\n",
      "           2       1.00      0.00      0.00        20\n",
      "           3       1.00      0.00      0.00         3\n",
      "           4       1.00      0.00      0.00      1231\n",
      "           5       1.00      0.00      0.00       133\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00       141\n",
      "           8       0.00      0.00      0.00         7\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      0.00      0.00       293\n",
      "          11       1.00      0.00      0.00       996\n",
      "          12       0.00      0.00      0.00        18\n",
      "          13       1.00      0.00      0.00        17\n",
      "          14       0.22      0.33      0.27      4656\n",
      "          15       0.00      0.00      0.00        73\n",
      "          16       0.21      0.19      0.20      9711\n",
      "          17       1.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         2\n",
      "          19       1.00      0.00      0.00        41\n",
      "          20       0.00      0.00      0.00       157\n",
      "          21       1.00      0.00      0.00       685\n",
      "          22       1.00      0.00      0.00        15\n",
      "          23       1.00      0.00      0.00        13\n",
      "          24       1.00      0.00      0.00       319\n",
      "          25       0.00      0.00      0.00       735\n",
      "          26       1.00      0.00      0.00        14\n",
      "          27       1.00      0.00      0.00       665\n",
      "          28       1.00      0.00      0.00       178\n",
      "          29       1.00      0.00      0.00       331\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       0.00      0.00      0.00        12\n",
      "          33       1.00      0.00      0.00         2\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       1.00      0.00      0.00       944\n",
      "          36       1.00      0.00      0.00         2\n",
      "          37       1.00      0.00      0.00         9\n",
      "          38       1.00      0.00      0.00         4\n",
      "          39       1.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.15     22543\n",
      "   macro avg       0.73      0.04      0.01     22543\n",
      "weighted avg       0.45      0.15      0.14     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FGSM Attack with Higher Perturbation Strength\n",
    "print(\"\\n--- FGSM Attack with Higher Epsilon ---\")\n",
    "for eps in [2.0, 3.0]:  # Test stronger perturbations\n",
    "    fgsm = FastGradientMethod(estimator=classifier, eps=eps)\n",
    "    X_test_fgsm_adv = fgsm.generate(x=X_test_scaled)\n",
    "    X_test_fgsm_adv = np.clip(X_test_fgsm_adv, -3, 3)  # Clip to valid range\n",
    "\n",
    "    # Evaluate FGSM\n",
    "    y_pred_fgsm_encoded = model.predict(X_test_fgsm_adv).argmax(axis=1)\n",
    "    fgsm_accuracy = accuracy_score(y_test_encoded, y_pred_fgsm_encoded)\n",
    "    print(f\"FGSM Adversarial Accuracy (eps={eps}):\", fgsm_accuracy)\n",
    "    print(\"FGSM Classification Report:\\n\", \n",
    "          classification_report(y_test_encoded, y_pred_fgsm_encoded,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ab67fa52-c2d8-4b2e-98db-8e65a31278ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PGD Attack with Increased Iterations ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256us/step\n",
      "PGD Adversarial Accuracy (stronger): 0.1356075056558577\n",
      "PGD Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       737\n",
      "           1       1.00      0.00      0.00       359\n",
      "           2       1.00      0.00      0.00        20\n",
      "           3       1.00      0.00      0.00         3\n",
      "           4       1.00      0.00      0.00      1231\n",
      "           5       1.00      0.00      0.00       133\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00       141\n",
      "           8       1.00      0.00      0.00         7\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      0.00      0.00       293\n",
      "          11       1.00      0.00      0.00       996\n",
      "          12       0.00      0.00      0.00        18\n",
      "          13       1.00      0.00      0.00        17\n",
      "          14       0.25      0.32      0.28      4656\n",
      "          15       0.00      0.00      0.00        73\n",
      "          16       0.20      0.16      0.18      9711\n",
      "          17       1.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         2\n",
      "          19       1.00      0.00      0.00        41\n",
      "          20       0.00      0.00      0.00       157\n",
      "          21       1.00      0.00      0.00       685\n",
      "          22       1.00      0.00      0.00        15\n",
      "          23       0.00      0.00      0.00        13\n",
      "          24       1.00      0.00      0.00       319\n",
      "          25       0.00      0.00      0.00       735\n",
      "          26       1.00      0.00      0.00        14\n",
      "          27       1.00      0.00      0.00       665\n",
      "          28       1.00      0.00      0.00       178\n",
      "          29       1.00      0.00      0.00       331\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       1.00      0.00      0.00        12\n",
      "          33       1.00      0.00      0.00         2\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       1.00      0.00      0.00       944\n",
      "          36       1.00      0.00      0.00         2\n",
      "          37       1.00      0.00      0.00         9\n",
      "          38       1.00      0.00      0.00         4\n",
      "          39       1.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.14     22543\n",
      "   macro avg       0.78      0.04      0.01     22543\n",
      "weighted avg       0.45      0.14      0.13     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PGD Attack with More Iterations\n",
    "print(\"\\n--- PGD Attack with Increased Iterations ---\")\n",
    "pgd = ProjectedGradientDescent(\n",
    "    estimator=classifier,\n",
    "    norm=np.inf,\n",
    "    eps=3.0,              # Increased epsilon\n",
    "    eps_step=0.02,        # Smaller step size for fine perturbations\n",
    "    max_iter=200,         # More iterations for stronger attack\n",
    "    targeted=False\n",
    ")\n",
    "X_test_pgd_adv = pgd.generate(x=X_test_scaled)\n",
    "X_test_pgd_adv = np.clip(X_test_pgd_adv, -3, 3)\n",
    "\n",
    "# Evaluate PGD\n",
    "y_pred_pgd_encoded = model.predict(X_test_pgd_adv).argmax(axis=1)\n",
    "pgd_accuracy = accuracy_score(y_test_encoded, y_pred_pgd_encoded)\n",
    "print(\"PGD Adversarial Accuracy (stronger):\", pgd_accuracy)\n",
    "print(\"PGD Classification Report:\\n\", \n",
    "      classification_report(y_test_encoded, y_pred_pgd_encoded,zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "158e8b4b-1fca-4dcf-a1c4-f33e0578a7da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, CarliniL2Method\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "\n",
    "# Assuming your model and data are already loaded and preprocessed:\n",
    "# model, X_test_scaled, y_test_encoded\n",
    "\n",
    "# Wrap the trained Keras model in an ART classifier\n",
    "classifier = TensorFlowV2Classifier(\n",
    "    model=model,\n",
    "    nb_classes=len(label_encoder.classes_),\n",
    "    input_shape=(X_train_scaled.shape[1],),\n",
    "    loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    ")\n",
    "\n",
    "# Critical features\n",
    "critical_features = [19, 23, 2, 26, 20]  # Indices of critical features\n",
    "\n",
    "# Create a static mask for the critical features\n",
    "static_mask = np.zeros_like(X_test_scaled)\n",
    "static_mask[:, critical_features] = 1  # Mask only the critical features\n",
    "\n",
    "# Function to apply the static mask\n",
    "def apply_static_mask(X_original, X_adv, mask):\n",
    "    \"\"\"\n",
    "    Applies a static mask to perturb only selected features.\n",
    "    Args:\n",
    "        X_original: The original input data.\n",
    "        X_adv: The adversarial examples.\n",
    "        mask: A mask indicating which features to perturb (1 = perturb, 0 = keep original).\n",
    "    Returns:\n",
    "        Masked adversarial examples.\n",
    "    \"\"\"\n",
    "    return X_original + (X_adv - X_original) * mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "868cd0b3-9866-4d98-8411-0010144727ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FGSM Attack with Static Mask ---\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283us/step\n",
      "FGSM Adversarial Accuracy (eps=3.0): 0.09688151532626536\n",
      "FGSM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       737\n",
      "           1       1.00      0.00      0.00       359\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       1.00      0.00      0.00         3\n",
      "           4       1.00      0.00      0.00      1231\n",
      "           5       1.00      0.00      0.00       133\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00       141\n",
      "           8       0.00      0.00      0.00         7\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      0.00      0.00       293\n",
      "          11       1.00      0.00      0.00       996\n",
      "          12       0.00      0.00      0.00        18\n",
      "          13       1.00      0.00      0.00        17\n",
      "          14       0.18      0.33      0.24      4656\n",
      "          15       0.00      0.00      0.00        73\n",
      "          16       0.11      0.06      0.08      9711\n",
      "          17       1.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         2\n",
      "          19       1.00      0.00      0.00        41\n",
      "          20       0.00      0.08      0.00       157\n",
      "          21       1.00      0.00      0.00       685\n",
      "          22       1.00      0.00      0.00        15\n",
      "          23       1.00      0.00      0.00        13\n",
      "          24       1.00      0.00      0.00       319\n",
      "          25       0.00      0.00      0.00       735\n",
      "          26       1.00      0.00      0.00        14\n",
      "          27       1.00      0.00      0.00       665\n",
      "          28       1.00      0.00      0.00       178\n",
      "          29       1.00      0.00      0.00       331\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       1.00      0.00      0.00        12\n",
      "          33       1.00      0.00      0.00         2\n",
      "          35       1.00      0.00      0.00       944\n",
      "          36       1.00      0.00      0.00         2\n",
      "          37       1.00      0.00      0.00         9\n",
      "          38       1.00      0.00      0.00         4\n",
      "          39       1.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.10     22543\n",
      "   macro avg       0.77      0.01      0.01     22543\n",
      "weighted avg       0.39      0.10      0.08     22543\n",
      "\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step\n",
      "FGSM Adversarial Accuracy (eps=4.0): 0.03504413786984873\n",
      "FGSM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       737\n",
      "           1       1.00      0.00      0.00       359\n",
      "           2       1.00      0.00      0.00        20\n",
      "           3       1.00      0.00      0.00         3\n",
      "           4       1.00      0.00      0.00      1231\n",
      "           5       1.00      0.00      0.00       133\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00       141\n",
      "           8       0.00      0.00      0.00         7\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      0.00      0.00       293\n",
      "          11       1.00      0.00      0.00       996\n",
      "          12       0.00      0.00      0.00        18\n",
      "          13       1.00      0.00      0.00        17\n",
      "          14       0.03      0.05      0.04      4656\n",
      "          15       0.00      0.00      0.00        73\n",
      "          16       0.08      0.06      0.07      9711\n",
      "          17       1.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         2\n",
      "          19       1.00      0.00      0.00        41\n",
      "          20       0.00      0.06      0.00       157\n",
      "          21       1.00      0.00      0.00       685\n",
      "          22       1.00      0.00      0.00        15\n",
      "          23       1.00      0.00      0.00        13\n",
      "          24       1.00      0.00      0.00       319\n",
      "          25       0.00      0.00      0.00       735\n",
      "          26       1.00      0.00      0.00        14\n",
      "          27       1.00      0.00      0.00       665\n",
      "          28       1.00      0.00      0.00       178\n",
      "          29       1.00      0.00      0.00       331\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       1.00      0.00      0.00        12\n",
      "          33       1.00      0.00      0.00         2\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       1.00      0.00      0.00       944\n",
      "          36       1.00      0.00      0.00         2\n",
      "          37       1.00      0.00      0.00         9\n",
      "          38       1.00      0.00      0.00         4\n",
      "          39       1.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.04     22543\n",
      "   macro avg       0.75      0.03      0.00     22543\n",
      "weighted avg       0.35      0.04      0.04     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FGSM Attack with Static Mask\n",
    "print(\"\\n--- FGSM Attack with Static Mask ---\")\n",
    "for eps in [3.0, 4.0]:  # Test with varying epsilon values\n",
    "    fgsm = FastGradientMethod(estimator=classifier, eps=eps)\n",
    "    X_test_fgsm_adv = fgsm.generate(x=X_test_scaled)\n",
    "    X_test_fgsm_adv = apply_static_mask(X_test_scaled, X_test_fgsm_adv, static_mask)\n",
    "    X_test_fgsm_adv = np.clip(X_test_fgsm_adv, -3, 3)  # Clip to valid range\n",
    "\n",
    "    # Evaluate FGSM with masking\n",
    "    y_pred_fgsm_encoded = model.predict(X_test_fgsm_adv).argmax(axis=1)\n",
    "    fgsm_accuracy = accuracy_score(y_test_encoded, y_pred_fgsm_encoded)\n",
    "    print(f\"FGSM Adversarial Accuracy (eps={eps}):\", fgsm_accuracy)\n",
    "    print(\"FGSM Classification Report:\\n\", \n",
    "          classification_report(y_test_encoded, y_pred_fgsm_encoded,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "580de0cd-2d12-463f-8aae-fa960c8fe7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PGD Attack with Static Mask ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 22:45:55.174217: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265us/step\n",
      "PGD Adversarial Accuracy (with masking): 0.20134853391296634\n",
      "PGD Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       737\n",
      "           1       1.00      0.00      0.00       359\n",
      "           2       1.00      0.00      0.00        20\n",
      "           3       1.00      0.00      0.00         3\n",
      "           4       1.00      0.00      0.00      1231\n",
      "           5       1.00      0.00      0.00       133\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       1.00      0.00      0.00       141\n",
      "           8       0.00      0.00      0.00         7\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      0.00      0.00       293\n",
      "          11       1.00      0.00      0.00       996\n",
      "          12       0.00      0.00      0.00        18\n",
      "          13       1.00      0.00      0.00        17\n",
      "          14       0.27      0.31      0.29      4656\n",
      "          15       0.00      0.00      0.00        73\n",
      "          16       0.31      0.32      0.31      9711\n",
      "          17       1.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         2\n",
      "          19       1.00      0.00      0.00        41\n",
      "          20       0.00      0.07      0.00       157\n",
      "          21       1.00      0.00      0.00       685\n",
      "          22       1.00      0.00      0.00        15\n",
      "          23       1.00      0.00      0.00        13\n",
      "          24       1.00      0.00      0.00       319\n",
      "          25       0.00      0.00      0.00       735\n",
      "          26       1.00      0.00      0.00        14\n",
      "          27       1.00      0.00      0.00       665\n",
      "          28       1.00      0.00      0.00       178\n",
      "          29       1.00      0.00      0.00       331\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       1.00      0.00      0.00        12\n",
      "          33       1.00      0.00      0.00         2\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       1.00      0.00      0.00       944\n",
      "          36       1.00      0.00      0.00         2\n",
      "          37       1.00      0.00      0.00         9\n",
      "          38       1.00      0.00      0.00         4\n",
      "          39       1.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.20     22543\n",
      "   macro avg       0.81      0.04      0.02     22543\n",
      "weighted avg       0.51      0.20      0.19     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PGD Attack with Static Mask\n",
    "print(\"\\n--- PGD Attack with Static Mask ---\")\n",
    "pgd = ProjectedGradientDescent(\n",
    "    estimator=classifier,\n",
    "    norm=np.inf,\n",
    "    eps=3.0,\n",
    "    eps_step=0.02,\n",
    "    max_iter=100,\n",
    "    targeted=False\n",
    ")\n",
    "X_test_pgd_adv = pgd.generate(x=X_test_scaled)\n",
    "X_test_pgd_adv = apply_static_mask(X_test_scaled, X_test_pgd_adv, static_mask)\n",
    "X_test_pgd_adv = np.clip(X_test_pgd_adv, -3, 3)\n",
    "\n",
    "\n",
    "# Evaluate PGD with masking\n",
    "y_pred_pgd_encoded = model.predict(X_test_pgd_adv).argmax(axis=1)\n",
    "pgd_accuracy = accuracy_score(y_test_encoded, y_pred_pgd_encoded)\n",
    "print(\"PGD Adversarial Accuracy (with masking):\", pgd_accuracy)\n",
    "print(\"PGD Classification Report:\\n\", \n",
    "      classification_report(y_test_encoded, y_pred_pgd_encoded,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "77706a0e-5077-41aa-93ac-336e2e05f20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Carlini & Wagner Attack with Static Mask ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c44f490cb894b8d8d7a58272c819d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step\n",
      "C&W Adversarial Accuracy (with masking): 0.205\n",
      "C&W Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         3\n",
      "           1       1.00      0.00      0.00         1\n",
      "           2       1.00      0.00      0.00         1\n",
      "           4       1.00      0.00      0.00        10\n",
      "           5       1.00      0.00      0.00         1\n",
      "           7       1.00      0.00      0.00         1\n",
      "          10       1.00      0.00      0.00         1\n",
      "          11       1.00      0.00      0.00        10\n",
      "          14       0.27      0.23      0.24        53\n",
      "          15       1.00      0.00      0.00         2\n",
      "          16       0.28      0.34      0.31        85\n",
      "          19       1.00      0.00      0.00         2\n",
      "          20       0.00      1.00      0.00         0\n",
      "          21       1.00      0.00      0.00        10\n",
      "          22       1.00      0.00      0.00         1\n",
      "          24       1.00      0.00      0.00         2\n",
      "          25       0.00      0.00      0.00         5\n",
      "          27       1.00      0.00      0.00         4\n",
      "          28       1.00      0.00      0.00         2\n",
      "          29       1.00      0.00      0.00         2\n",
      "          34       0.00      1.00      0.00         0\n",
      "          35       1.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.20       200\n",
      "   macro avg       0.80      0.12      0.03       200\n",
      "weighted avg       0.47      0.20      0.19       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Carlini & Wagner Attack with Static Mask ---\")\n",
    "cw = CarliniL2Method(\n",
    "    classifier=classifier,\n",
    "    confidence=2.0,\n",
    "    targeted=False,\n",
    "    max_iter=100,\n",
    "    learning_rate=0.01,\n",
    "    binary_search_steps=5\n",
    ")\n",
    "X_test_cw_adv = cw.generate(x=X_test_scaled[:200])  # Subset for C&W due to computational cost\n",
    "X_test_cw_adv = apply_static_mask(X_test_scaled[:200], X_test_cw_adv, static_mask[:200])\n",
    "X_test_cw_adv = np.clip(X_test_cw_adv, -3, 3)\n",
    "\n",
    "# Evaluate C&W with masking\n",
    "y_pred_cw_encoded = model.predict(X_test_cw_adv).argmax(axis=1)\n",
    "cw_accuracy = accuracy_score(y_test_encoded[:200], y_pred_cw_encoded)\n",
    "print(\"C&W Adversarial Accuracy (with masking):\", cw_accuracy)\n",
    "print(\"C&W Classification Report:\\n\", \n",
    "      classification_report(y_test_encoded[:200], y_pred_cw_encoded,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3433cdf7-48e1-4882-88eb-58bd0bd97c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes: 40\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 435us/step - accuracy: 0.9322 - loss: 0.2611 - val_accuracy: 0.3338 - val_loss: 5.7909\n",
      "Epoch 2/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 423us/step - accuracy: 0.9673 - loss: 0.0933 - val_accuracy: 0.4062 - val_loss: 6.0013\n",
      "Epoch 3/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 447us/step - accuracy: 0.9676 - loss: 0.0913 - val_accuracy: 0.2329 - val_loss: 6.8171\n",
      "Epoch 4/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 449us/step - accuracy: 0.9712 - loss: 0.0795 - val_accuracy: 0.3936 - val_loss: 6.7951\n",
      "Epoch 5/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 449us/step - accuracy: 0.9738 - loss: 0.0741 - val_accuracy: 0.3312 - val_loss: 8.4143\n",
      "Epoch 6/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 442us/step - accuracy: 0.9771 - loss: 0.0676 - val_accuracy: 0.0919 - val_loss: 9.1451\n",
      "Epoch 7/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 419us/step - accuracy: 0.9780 - loss: 0.0653 - val_accuracy: 0.2181 - val_loss: 9.7639\n",
      "Epoch 8/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 435us/step - accuracy: 0.9790 - loss: 0.0607 - val_accuracy: 0.2906 - val_loss: 9.5139\n",
      "Epoch 9/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 424us/step - accuracy: 0.9797 - loss: 0.0600 - val_accuracy: 0.2902 - val_loss: 12.4981\n",
      "Epoch 10/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 435us/step - accuracy: 0.9802 - loss: 0.0587 - val_accuracy: 0.2229 - val_loss: 13.2600\n",
      "Epoch 11/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 445us/step - accuracy: 0.9811 - loss: 0.0558 - val_accuracy: 0.1178 - val_loss: 11.5307\n",
      "Epoch 12/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 436us/step - accuracy: 0.9817 - loss: 0.0550 - val_accuracy: 0.1272 - val_loss: 12.3212\n",
      "Epoch 13/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 429us/step - accuracy: 0.9810 - loss: 0.0563 - val_accuracy: 0.2766 - val_loss: 14.9801\n",
      "Epoch 14/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 430us/step - accuracy: 0.9803 - loss: 0.0569 - val_accuracy: 0.0965 - val_loss: 19.1128\n",
      "Epoch 15/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 440us/step - accuracy: 0.9810 - loss: 0.0549 - val_accuracy: 0.2883 - val_loss: 14.9998\n",
      "Epoch 16/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 431us/step - accuracy: 0.9810 - loss: 0.0551 - val_accuracy: 0.3048 - val_loss: 14.7477\n",
      "Epoch 17/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 427us/step - accuracy: 0.9817 - loss: 0.0532 - val_accuracy: 0.1206 - val_loss: 18.7053\n",
      "Epoch 18/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 436us/step - accuracy: 0.9815 - loss: 0.0557 - val_accuracy: 0.1010 - val_loss: 20.7156\n",
      "Epoch 19/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 432us/step - accuracy: 0.9800 - loss: 0.0572 - val_accuracy: 0.1046 - val_loss: 21.9196\n",
      "Epoch 20/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 448us/step - accuracy: 0.9817 - loss: 0.0521 - val_accuracy: 0.1155 - val_loss: 24.7977\n",
      "Epoch 21/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 451us/step - accuracy: 0.9806 - loss: 0.0554 - val_accuracy: 0.1037 - val_loss: 23.6986\n",
      "Epoch 22/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 448us/step - accuracy: 0.9817 - loss: 0.0544 - val_accuracy: 0.2343 - val_loss: 19.6247\n",
      "Epoch 23/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 435us/step - accuracy: 0.9824 - loss: 0.0519 - val_accuracy: 0.2226 - val_loss: 21.2569\n",
      "Epoch 24/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 426us/step - accuracy: 0.9821 - loss: 0.0521 - val_accuracy: 0.1033 - val_loss: 20.3650\n",
      "Epoch 25/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 425us/step - accuracy: 0.9826 - loss: 0.0517 - val_accuracy: 0.1104 - val_loss: 22.6576\n",
      "Epoch 26/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 427us/step - accuracy: 0.9819 - loss: 0.0520 - val_accuracy: 0.2867 - val_loss: 28.1588\n",
      "Epoch 27/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 426us/step - accuracy: 0.9820 - loss: 0.0509 - val_accuracy: 0.2403 - val_loss: 25.7116\n",
      "Epoch 28/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 425us/step - accuracy: 0.9820 - loss: 0.0505 - val_accuracy: 0.1139 - val_loss: 34.3904\n",
      "Epoch 29/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 426us/step - accuracy: 0.9828 - loss: 0.0503 - val_accuracy: 0.2923 - val_loss: 34.6605\n",
      "Epoch 30/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 428us/step - accuracy: 0.9817 - loss: 0.0520 - val_accuracy: 0.1348 - val_loss: 26.6158\n",
      "Epoch 31/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 441us/step - accuracy: 0.9836 - loss: 0.0491 - val_accuracy: 0.2877 - val_loss: 32.6290\n",
      "Epoch 32/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 453us/step - accuracy: 0.9838 - loss: 0.0489 - val_accuracy: 0.2335 - val_loss: 37.8339\n",
      "Epoch 33/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 448us/step - accuracy: 0.9828 - loss: 0.0494 - val_accuracy: 0.1081 - val_loss: 36.7841\n",
      "Epoch 34/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 450us/step - accuracy: 0.9832 - loss: 0.0494 - val_accuracy: 0.3384 - val_loss: 41.0072\n",
      "Epoch 35/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 430us/step - accuracy: 0.9829 - loss: 0.0494 - val_accuracy: 0.2286 - val_loss: 29.1188\n",
      "Epoch 36/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 446us/step - accuracy: 0.9818 - loss: 0.0523 - val_accuracy: 0.2214 - val_loss: 33.7222\n",
      "Epoch 37/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 440us/step - accuracy: 0.9832 - loss: 0.0493 - val_accuracy: 0.3358 - val_loss: 41.8608\n",
      "Epoch 38/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 441us/step - accuracy: 0.9833 - loss: 0.0489 - val_accuracy: 0.2164 - val_loss: 35.9489\n",
      "Epoch 39/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 438us/step - accuracy: 0.9835 - loss: 0.0474 - val_accuracy: 0.2075 - val_loss: 48.7222\n",
      "Epoch 40/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 438us/step - accuracy: 0.9837 - loss: 0.0479 - val_accuracy: 0.3350 - val_loss: 59.0221\n",
      "Epoch 41/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 457us/step - accuracy: 0.9823 - loss: 0.0501 - val_accuracy: 0.2885 - val_loss: 44.8004\n",
      "Epoch 42/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 435us/step - accuracy: 0.9831 - loss: 0.0480 - val_accuracy: 0.1651 - val_loss: 49.7862\n",
      "Epoch 43/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 454us/step - accuracy: 0.9832 - loss: 0.0494 - val_accuracy: 0.2708 - val_loss: 57.4694\n",
      "Epoch 44/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 440us/step - accuracy: 0.9845 - loss: 0.0464 - val_accuracy: 0.1118 - val_loss: 60.8754\n",
      "Epoch 45/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 433us/step - accuracy: 0.9840 - loss: 0.0463 - val_accuracy: 0.2205 - val_loss: 52.4860\n",
      "Epoch 46/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 430us/step - accuracy: 0.9837 - loss: 0.0493 - val_accuracy: 0.3423 - val_loss: 67.0042\n",
      "Epoch 47/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 435us/step - accuracy: 0.9831 - loss: 0.0501 - val_accuracy: 0.2837 - val_loss: 55.7509\n",
      "Epoch 48/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 430us/step - accuracy: 0.9830 - loss: 0.0489 - val_accuracy: 0.3480 - val_loss: 62.5969\n",
      "Epoch 49/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 458us/step - accuracy: 0.9850 - loss: 0.0458 - val_accuracy: 0.2172 - val_loss: 60.8151\n",
      "Epoch 50/50\n",
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 440us/step - accuracy: 0.9843 - loss: 0.0472 - val_accuracy: 0.2957 - val_loss: 55.0276\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assume X_train and y_train are your training data (features and labels), already preprocessed\n",
    "# Assume X_test and y_test are your test data (features and labels), already preprocessed\n",
    "\n",
    "# Step 1: Scale the Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Apply QuantileTransformer to transform features to a normal distribution\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "X_train_scaled = quantile_transformer.fit_transform(X_train)\n",
    "X_test_scaled = quantile_transformer.transform(X_test)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Concatenate y_train and y_test to include all labels\n",
    "all_labels = np.concatenate([y_train, y_test])\n",
    "\n",
    "# Fit LabelEncoder on the combined labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Transform y_train and y_test using the fitted encoder\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "num_classes = len(np.unique(all_labels))  # Total number of unique classes in y_train and y_test\n",
    "print(\"Number of unique classes:\", num_classes)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Redefine the model with the correct number of classes\n",
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')  # Ensure output layer matches the number of unique classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "mlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the Model\n",
    "history = mlp_model.fit(X_train_scaled, y_train_encoded, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1fd26d7-c38a-40ea-bde5-6669420bf0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280us/step\n",
      "MLP Model Accuracy on Clean Data: 0.2957459078206095\n",
      "Classification Report on Clean Data with Attack Names:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        apache2       1.00      0.00      0.00       737\n",
      "           back       1.00      0.00      0.00       359\n",
      "buffer_overflow       1.00      0.00      0.00        20\n",
      "      ftp_write       1.00      0.00      0.00         3\n",
      "   guess_passwd       0.00      0.00      0.00      1231\n",
      "     httptunnel       1.00      0.00      0.00       133\n",
      "           imap       1.00      0.00      0.00         1\n",
      "        ipsweep       0.00      0.00      0.00       141\n",
      "           land       1.00      0.00      0.00         7\n",
      "     loadmodule       1.00      0.00      0.00         2\n",
      "       mailbomb       1.00      0.00      0.00       293\n",
      "          mscan       1.00      0.00      0.00       996\n",
      "       multihop       0.00      0.00      0.00        18\n",
      "          named       1.00      0.00      0.00        17\n",
      "        neptune       0.34      0.34      0.34      4656\n",
      "           nmap       0.00      0.00      0.00        73\n",
      "         normal       0.49      0.48      0.49      9711\n",
      "           perl       1.00      0.00      0.00         2\n",
      "            phf       1.00      0.00      0.00         2\n",
      "            pod       1.00      0.00      0.00        41\n",
      "      portsweep       0.03      0.95      0.06       157\n",
      "   processtable       1.00      0.00      0.00       685\n",
      "             ps       1.00      0.00      0.00        15\n",
      "        rootkit       0.00      0.00      0.00        13\n",
      "          saint       1.00      0.00      0.00       319\n",
      "          satan       0.09      0.40      0.15       735\n",
      "       sendmail       1.00      0.00      0.00        14\n",
      "          smurf       1.00      0.00      0.00       665\n",
      "  snmpgetattack       1.00      0.00      0.00       178\n",
      "      snmpguess       1.00      0.00      0.00       331\n",
      "      sqlattack       1.00      0.00      0.00         2\n",
      "       teardrop       1.00      0.00      0.00        12\n",
      "       udpstorm       1.00      0.00      0.00         2\n",
      "    warezmaster       1.00      0.00      0.00       944\n",
      "           worm       1.00      0.00      0.00         2\n",
      "          xlock       0.00      0.00      0.00         9\n",
      "         xsnoop       1.00      0.00      0.00         4\n",
      "          xterm       1.00      0.00      0.00        13\n",
      "\n",
      "       accuracy                           0.30     22543\n",
      "      macro avg       0.76      0.06      0.03     22543\n",
      "   weighted avg       0.54      0.30      0.28     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the MLP model on clean data using encoded labels for y_test\n",
    "y_pred = mlp_model.predict(X_test_scaled).argmax(axis=1)\n",
    "clean_accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "clean_class_report = classification_report(y_test_encoded, y_pred, zero_division=1)\n",
    "clean_conf_matrix = confusion_matrix(y_test_encoded, y_pred)\n",
    "# Decode the integer labels back to the original class names\n",
    "y_test_labels = label_encoder.inverse_transform(y_test_encoded)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Generate the classification report with the original attack names\n",
    "decoded_class_report = classification_report(y_test_labels, y_pred_labels, zero_division=1)\n",
    "decoded_conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "print(\"MLP Model Accuracy on Clean Data:\", clean_accuracy)\n",
    "print(\"Classification Report on Clean Data with Attack Names:\\n\", decoded_class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf4ce2af-a6e1-48d4-9364-bae0bbb6b9e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, CarliniL2Method\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "\n",
    "# Wrap the trained MLP model in an ART classifier\n",
    "classifier = TensorFlowV2Classifier(\n",
    "    model=mlp_model,\n",
    "    nb_classes=len(label_encoder.classes_),\n",
    "    input_shape=(X_train_scaled.shape[1],),\n",
    "    loss_object=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    ")\n",
    "\n",
    "# Define the critical features\n",
    "critical_features = [19, 23, 26, 2, 20, 27]  # From SHAP analysis\n",
    "\n",
    "# Create a dynamic mask targeting the critical features\n",
    "def create_static_mask(data, critical_features):\n",
    "    \"\"\"\n",
    "    Creates a mask with 1s for critical features and 0s elsewhere.\n",
    "    Args:\n",
    "        data: Dataset (numpy array or DataFrame).\n",
    "        critical_features: List of critical feature indices.\n",
    "    Returns:\n",
    "        Mask of the same shape as data, with 1s for critical features.\n",
    "    \"\"\"\n",
    "    mask = np.zeros_like(data)  # Initialize a mask of zeros\n",
    "    mask[:, critical_features] = 1  # Target critical features\n",
    "    return mask\n",
    "\n",
    "# Generate the static mask for the test data\n",
    "static_mask = create_static_mask(X_test_scaled, critical_features)\n",
    "\n",
    "# Apply the mask to adversarial examples\n",
    "def apply_static_mask(X_original, X_adv, mask):\n",
    "    \"\"\"\n",
    "    Applies a static mask to perturb only selected features.\n",
    "    Args:\n",
    "        X_original: Original input data.\n",
    "        X_adv: Adversarial examples.\n",
    "        mask: Mask indicating which features to perturb (1 = perturb, 0 = keep original).\n",
    "    Returns:\n",
    "        Masked adversarial examples.\n",
    "    \"\"\"\n",
    "    return X_original + (X_adv - X_original) * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9680eae8-53cc-4128-be54-970ac5bcb3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FGSM Attack with Dynamic Masking ---\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step\n",
      "FGSM Adversarial Accuracy (eps=3.0): 0.33815375060994546\n",
      "FGSM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       737\n",
      "           1       0.00      0.00      0.00       359\n",
      "           2       1.00      0.00      0.00        20\n",
      "           3       1.00      0.00      0.00         3\n",
      "           4       1.00      0.00      0.00      1231\n",
      "           5       1.00      0.00      0.00       133\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00       141\n",
      "           8       0.00      0.00      0.00         7\n",
      "           9       1.00      0.00      0.00         2\n",
      "          10       1.00      0.00      0.00       293\n",
      "          11       1.00      0.00      0.00       996\n",
      "          12       1.00      0.00      0.00        18\n",
      "          13       1.00      0.00      0.00        17\n",
      "          14       0.30      0.31      0.31      4656\n",
      "          15       0.00      0.00      0.00        73\n",
      "          16       0.55      0.62      0.58      9711\n",
      "          17       1.00      0.00      0.00         2\n",
      "          18       1.00      0.00      0.00         2\n",
      "          19       1.00      0.00      0.00        41\n",
      "          20       0.03      0.85      0.05       157\n",
      "          21       1.00      0.00      0.00       685\n",
      "          22       1.00      0.00      0.00        15\n",
      "          23       1.00      0.00      0.00        13\n",
      "          24       1.00      0.00      0.00       319\n",
      "          25       0.00      0.00      0.00       735\n",
      "          26       1.00      0.00      0.00        14\n",
      "          27       0.00      0.00      0.00       665\n",
      "          28       1.00      0.00      0.00       178\n",
      "          29       1.00      0.00      0.00       331\n",
      "          31       1.00      0.00      0.00         2\n",
      "          32       1.00      0.00      0.00        12\n",
      "          33       1.00      0.00      0.00         2\n",
      "          35       1.00      0.00      0.00       944\n",
      "          36       1.00      0.00      0.00         2\n",
      "          37       1.00      0.00      0.00         9\n",
      "          38       1.00      0.00      0.00         4\n",
      "          39       1.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.34     22543\n",
      "   macro avg       0.79      0.05      0.02     22543\n",
      "weighted avg       0.57      0.34      0.32     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FGSM Attack with Dynamic Masking\n",
    "print(\"\\n--- FGSM Attack with Dynamic Masking ---\")\n",
    "for eps in [3.0]:  # Test with varying epsilon values\n",
    "    fgsm = FastGradientMethod(estimator=classifier, eps=eps)\n",
    "    X_test_fgsm_adv = fgsm.generate(x=X_test_scaled)\n",
    "    X_test_fgsm_adv = apply_static_mask(X_test_scaled, X_test_fgsm_adv, static_mask)\n",
    "    X_test_fgsm_adv = np.clip(X_test_fgsm_adv, -3, 3)  # Clip to valid range\n",
    "\n",
    "    # Evaluate FGSM with masking\n",
    "    y_pred_fgsm_encoded = mlp_model.predict(X_test_fgsm_adv).argmax(axis=1)\n",
    "    fgsm_accuracy = accuracy_score(y_test_encoded, y_pred_fgsm_encoded)\n",
    "    print(f\"FGSM Adversarial Accuracy (eps={eps}):\", fgsm_accuracy)\n",
    "    print(\"FGSM Classification Report:\\n\", \n",
    "          classification_report(y_test_encoded, y_pred_fgsm_encoded,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11210d9d-b83c-4be8-8080-d7040b2e2ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1cd1ae35-e6f9-4f46-a01e-3174ed6f38b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PGD Attack with Dynamic Masking ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267us/step\n",
      "PGD Adversarial Accuracy (with masking): 0.36068846205030386\n",
      "PGD Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       737\n",
      "           1       0.00      0.00      0.00       359\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00      1231\n",
      "           5       0.00      0.00      0.00       133\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00       141\n",
      "           8       0.00      0.00      0.00         7\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       0.00      0.00      0.00       293\n",
      "          11       0.00      0.00      0.00       996\n",
      "          12       0.00      0.00      0.00        18\n",
      "          13       0.00      0.00      0.00        17\n",
      "          14       0.62      0.31      0.41      4656\n",
      "          15       0.00      0.00      0.00        73\n",
      "          16       0.45      0.67      0.54      9711\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.00      0.00      0.00        41\n",
      "          20       0.00      0.02      0.00       157\n",
      "          21       0.00      0.00      0.00       685\n",
      "          22       0.00      0.00      0.00        15\n",
      "          23       0.00      0.00      0.00        13\n",
      "          24       0.00      0.00      0.00       319\n",
      "          25       0.05      0.24      0.08       735\n",
      "          26       0.00      0.00      0.00        14\n",
      "          27       0.00      0.00      0.00       665\n",
      "          28       0.00      0.00      0.00       178\n",
      "          29       0.00      0.00      0.00       331\n",
      "          31       0.00      0.00      0.00         2\n",
      "          32       0.00      0.00      0.00        12\n",
      "          33       0.00      0.00      0.00         2\n",
      "          35       0.00      0.00      0.00       944\n",
      "          36       0.00      0.00      0.00         2\n",
      "          37       0.00      0.00      0.00         9\n",
      "          38       0.00      0.00      0.00         4\n",
      "          39       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.36     22543\n",
      "   macro avg       0.03      0.03      0.03     22543\n",
      "weighted avg       0.32      0.36      0.32     22543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PGD Attack with Dynamic Masking\n",
    "print(\"\\n--- PGD Attack with Dynamic Masking ---\")\n",
    "pgd = ProjectedGradientDescent(\n",
    "    estimator=classifier,\n",
    "    norm=np.inf,\n",
    "    eps=3.0,\n",
    "    eps_step=0.02,\n",
    "    max_iter=100,\n",
    "    targeted=False\n",
    ")\n",
    "X_test_pgd_adv = pgd.generate(x=X_test_scaled)\n",
    "X_test_pgd_adv = apply_static_mask(X_test_scaled, X_test_pgd_adv, static_mask)\n",
    "X_test_pgd_adv = np.clip(X_test_pgd_adv, -3, 3)\n",
    "\n",
    "# Evaluate PGD with masking\n",
    "y_pred_pgd_encoded = mlp_model.predict(X_test_pgd_adv).argmax(axis=1)\n",
    "pgd_accuracy = accuracy_score(y_test_encoded, y_pred_pgd_encoded)\n",
    "print(\"PGD Adversarial Accuracy (with masking):\", pgd_accuracy)\n",
    "print(\"PGD Classification Report:\\n\", \n",
    "      classification_report(y_test_encoded, y_pred_pgd_encoded,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93cf1da3-156d-4881-8fdc-2958d02ac3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Carlini & Wagner Attack with Dynamic Masking ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103ec056f19648f39ff6be71091255a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_2:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step\n",
      "C&W Adversarial Accuracy (with masking): 0.27\n",
      "C&W Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         3\n",
      "           1       1.00      0.00      0.00         1\n",
      "           2       1.00      0.00      0.00         1\n",
      "           4       1.00      0.00      0.00        10\n",
      "           5       1.00      0.00      0.00         1\n",
      "           7       1.00      0.00      0.00         1\n",
      "          10       1.00      0.00      0.00         1\n",
      "          11       1.00      0.00      0.00        10\n",
      "          14       0.31      0.23      0.26        53\n",
      "          15       1.00      0.00      0.00         2\n",
      "          16       0.53      0.49      0.51        85\n",
      "          19       1.00      0.00      0.00         2\n",
      "          20       0.00      1.00      0.00         0\n",
      "          21       1.00      0.00      0.00        10\n",
      "          22       1.00      0.00      0.00         1\n",
      "          24       1.00      0.00      0.00         2\n",
      "          25       0.00      0.00      0.00         5\n",
      "          27       1.00      0.00      0.00         4\n",
      "          28       1.00      0.00      0.00         2\n",
      "          29       1.00      0.00      0.00         2\n",
      "          35       1.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.27       200\n",
      "   macro avg       0.85      0.08      0.04       200\n",
      "weighted avg       0.59      0.27      0.29       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C&W Attack with Dynamic Masking\n",
    "print(\"\\n--- Carlini & Wagner Attack with Dynamic Masking ---\")\n",
    "cw = CarliniL2Method(\n",
    "    classifier=classifier,\n",
    "    confidence=2.0,\n",
    "    targeted=False,\n",
    "    max_iter=100,\n",
    "    learning_rate=0.01,\n",
    "    binary_search_steps=5\n",
    ")\n",
    "X_test_cw_adv = cw.generate(x=X_test_scaled[:200])  # Subset for C&W due to computational cost\n",
    "X_test_cw_adv = apply_static_mask(X_test_scaled[:200], X_test_cw_adv, static_mask[:200])\n",
    "X_test_cw_adv = np.clip(X_test_cw_adv, -3, 3)\n",
    "\n",
    "# Evaluate C&W with masking\n",
    "y_pred_cw_encoded = mlp_model.predict(X_test_cw_adv).argmax(axis=1)\n",
    "cw_accuracy = accuracy_score(y_test_encoded[:200], y_pred_cw_encoded)\n",
    "print(\"C&W Adversarial Accuracy (with masking):\", cw_accuracy)\n",
    "print(\"C&W Classification Report:\\n\", \n",
    "      classification_report(y_test_encoded[:200], y_pred_cw_encoded,zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c78224-3441-4b7d-948d-178f58c7d5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
